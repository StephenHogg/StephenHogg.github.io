<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Another way to look at risk versus reward in binary classification | It definitely worked on my computer</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Another way to look at risk versus reward in binary classification" />
<meta name="author" content="Stephen Hogg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production. At worst, this is deluded." />
<meta property="og:description" content="Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production. At worst, this is deluded." />
<link rel="canonical" href="http://localhost:4000/2019/07/10/Risk_in_Classification.html" />
<meta property="og:url" content="http://localhost:4000/2019/07/10/Risk_in_Classification.html" />
<meta property="og:site_name" content="It definitely worked on my computer" />
<meta property="og:image" content="http://localhost:4000/assets/img/this_is_fine.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-07-10T00:00:00+10:00" />
<script type="application/ld+json">
{"description":"Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production. At worst, this is deluded.","author":{"@type":"Person","name":"Stephen Hogg"},"@type":"BlogPosting","url":"http://localhost:4000/2019/07/10/Risk_in_Classification.html","image":"http://localhost:4000/assets/img/this_is_fine.png","headline":"Another way to look at risk versus reward in binary classification","dateModified":"2019-07-10T00:00:00+10:00","datePublished":"2019-07-10T00:00:00+10:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/07/10/Risk_in_Classification.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="It definitely worked on my computer" /><!-- Twitter cards -->
    <meta name="twitter:site" content="@whistle_posse">
    <meta name="twitter:creator" content="@whistle_posse">

    
    <meta name="twitter:title" content="Another way to look at risk versus reward in binary classification">
    

    
    <meta name="twitter:description" content="This blog contains wild-eyed rants and  things that I need to write down somewhere.  Obviously, the views expressed here are my own.
">
    

    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/this_is_fine.png">
    
    <!-- end of Twitter cards -->
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">It definitely worked on my computer</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About me</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Another way to look at risk versus reward in binary classification</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-07-10T00:00:00+10:00" itemprop="datePublished">Jul 10, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production. At worst, this is deluded.</p>

<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<p><img src="/assets/img/this_is_fine.png" width="800" /></p>

<p>So why don’t we try and make a guess at how much lower our performance might turn out to be? In this post, I’ll look at one way of doing this with a binary classifier. If you don’t want to worry about the details, <a href="#the-bigger-picture">skip to the final section</a>.</p>

<p>Of course, the worst case is that your model gets everything wrong. That’s way off in the tails of your likely outcomes and not the point of this exercise, though. What I’m aiming for here is a way of looking at how much a model might underperform in reality.</p>

<h1 id="gotta-risk-it-to-get-the-biscuit">Gotta risk it to get the biscuit</h1>

<p>Before you do anything, you need a decent baseline to compare to. What might we do in the absence of a model? You could flip a coin. Or you could always predict the commonest class. You could even use another much simpler model.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p>For now, let’s say that your baseline model is constant-valued. The model predicts all data are from the majority class.<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> So what does this mean for performance?</p>

<p>Imagine you’ve got a dataset where 90% of the labels are negative and the remaining 10% are positive. The constant-valued classifier gives you 90% accuracy. So if you do any worse than that on a model you’ve built, it’s fair to claim that you’ve wasted your time.</p>

<p>This is where things get interesting. We can be pretty confident about that 90% accuracy figure, because the model is simple. The more complex your model is though, the more open you are to things getting funky.<sup id="fnref:4"><a href="#fn:4" class="footnote">3</a></sup></p>

<p>So it’s reasonable to ask yourself:</p>

<blockquote>
  <p>“Given the extra performance above a baseline model I see, how much risk do I take in exchange?”</p>
</blockquote>

<p>…because after all, there ain’t no such thing as a free lunch!</p>

<h1 id="down-on-the-upside">Down on the upside<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup></h1>

<p>It’s easy to work out how much upside you’re capturing relative to a baseline. All you do is subtract the baseline accuracy you’ve got from your model’s reported test accuracy. If that comes out negative, stop reading: you have bigger problems.</p>

<h1 id="this-is-where-it-gets-confusing">This is where it gets confusing</h1>

<p>Now how much risk are you taking on to get the outperformance you see? One way of looking at this involves <a href="https://breakitdownto.earth/2019/06/12/Automated_Mistakes_with_AutoML.html#confused-yet">model perplexity (brief explanation you should read here)</a>. The question is, how to arrive at that measure for a binary classifier. The good news is that it’s not hard. It’s easy to turn binary cross-entropy into a perplexity score.</p>

<p>Wikipedia informs us that<sup id="fnref:6"><a href="#fn:6" class="footnote">5</a></sup>:</p>

<blockquote>
  <p>The perplexity of a discrete probability distribution p is defined as:</p>

  <script type="math/tex; mode=display">2^{H(p)}=2^{-\sum _{x=1} p(x)\log _{2}p(x)}</script>
</blockquote>

<p>The bit on the left side is the important thing here. Perplexity is equal to a base raised to the power of entropy. The base needs to be the same as the one used calculating entropy. So if you used the natural log when calculating cross-entropy, all you need to do is:</p>

<script type="math/tex; mode=display">perplexity = e^{loss}</script>

<p>and you’ve got a perplexity score. Perplexity can become an estimated accuracy score, too. If you know that only a couple of outcomes are plausible, then at worst you can make a random guess among them. This means that all you have to do is this:</p>

<script type="math/tex; mode=display">ACC_{lowerbound} = \frac{1}{e^{loss}}</script>

<p>So just exponentiate your loss and then divide one by it. Done.</p>

<h1 id="the-bigger-picture">The bigger picture</h1>

<p>So we now have three numbers to work with:</p>

<ol>
  <li>The accuracy as reported on our test set</li>
  <li>The accuracy achieved by a baseline model</li>
  <li>The estimate we have of how badly our model might plausibly perform in the wild derived above</li>
</ol>

<p>Now it’s just a matter of judgement really. Given how well your model appears to perform, is the risk on the downside worth it?</p>

<p>Here’s a scenario. You have a model with the following results:</p>

<ol>
  <li>Reported test set accuracy of 93% (implying 3% of upside on a baseline classifier)</li>
  <li>A baseline accuracy of 90%</li>
  <li>A lower bound estimate for accuracy of 75% (implying quite a lot of downside)</li>
</ol>

<p>It looks like the model takes on quite a bit of risk for the upside it appears to be capturing, so you may think twice about putting this model into production. It’s not hard to think about another situation where the worst case accuracy is not far below a random baseline.</p>

<p>None of this is all that revolutionary. The aim is to think explicitly about how much trouble a model might cause in production. That’s really the big deal here. Treating models like software means being clear-eyed about how they fail like software.</p>

<p>Finally, you can do this with other metrics aside from accuracy with some effort. Multi-class problems can also be addressed with a certain amount of jiggery pokery.</p>

<p>Hooray!</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Big caveat with this last one. If you’re not respecting the word “simple”, it probably shouldn’t be a baseline. The idea is to work out how much better of you are compared to no model. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>You could use something else, though. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>This is basically how adversarial attacks on ML models have become a thing. The more complicated models get, the more room there is for this. But this is an aside. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Yup. Soundgarden jokes. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>https://en.wikipedia.org/wiki/Perplexity <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/2019/07/10/Risk_in_Classification.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">It definitely worked on my computer</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Stephen Hogg</li><li><a class="u-email" href="mailto:saenet.r@gmail.com">saenet.r@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/StephenHogg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">StephenHogg</span></a></li><li><a href="https://www.linkedin.com/in/stephen-hogg-80109372"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">stephen-hogg-80109372</span></a></li><li><a href="https://www.twitter.com/whistle_posse"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">whistle_posse</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This blog contains wild-eyed rants and  things that I need to write down somewhere.  Obviously, the views expressed here are my own.
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
