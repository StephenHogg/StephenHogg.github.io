---
layout: post
title: "How to gather a bunch of evidence and then completely ignore it"
---

Oh my god this happens all the time I swear. If you're working in machine learning then it is unavoidable that you work in the presence of uncertainty. This isn't such a bad thing, but how we think as humans sometimes works against us when dealing with uncertainty. This post will look at what goes wrong and more importantly, how to spot it so you can do something about it.

Everyone's different[^1], so maybe this advice won't work for everyone. But that's fine.

# Getting your story straight

One obvious way to get caught is in a [narrative fallacy](https://wiki.lesswrong.com/wiki/Narrative_fallacy). And fair enough. Machine learning models can be big complicated things, so knowing what's going on in them is an active field of research in its own right, not just an onerous task for the practitioner. The easiest way to catch yourself succumbing to a narrative fallacy is if you ever say _"I know what's going on here"_, or words to that effect to yourself. 

That may sound like a sweeping generalisation, but it kind of isn't if you're working with anything beyond a very simple model and very simple data. It turns up a lot when you're iterating on a model to find a version that works well, because in order to figure out how to build something out you need some kind of rationale for what you're doing. [AutoML seems like a good way around this problem, but it's an iron law that in any situation a sufficiently innovative and eager person can always find a way to shoot themselves in the foot.]()


* Narrative fallacy
* Dunning Kruger effect
* 
* What can you do about it?
*


[^1]: of course
