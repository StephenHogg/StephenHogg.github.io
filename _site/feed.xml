<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-25T14:35:30+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">It definitely worked on my computer</title><subtitle>This blog contains wild-eyed rants and  things that I need to write down somewhere.  Obviously, the views expressed here are my own.
</subtitle><author><name>Stephen Hogg</name></author><entry><title type="html">Things I’ve learnt from one-and-a-bit years of Duolingo</title><link href="http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo.html" rel="alternate" type="text/html" title="Things I've learnt from one-and-a-bit years of Duolingo" /><published>2019-06-20T00:00:00+10:00</published><updated>2019-06-20T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo</id><content type="html" xml:base="http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo.html">&lt;p&gt;I’ve been using Duolingo for about 470 days now to learn Chinese. There are a few things I’ve learned on the way, but mostly I’ve learned to apply the lessons of a book I read partway through this journey. In this post, I’ll talk about that and what it leads me to think about how to turn this blog into a durable habit.&lt;/p&gt;

&lt;h1 id=&quot;pretty-clear-i-guess&quot;&gt;Pretty clear, I guess&lt;/h1&gt;

&lt;p&gt;The book is &lt;em&gt;Atomic Habits&lt;/em&gt; by James Clear.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The basic premise of the book is that we can improve our lives quite a lot by making minor changes that stack up over time and that the best way to do that involves reducing our reliance on motivation and will.&lt;/p&gt;

&lt;p&gt;All pretty intuitive so far, but what does this mean in practice? Well:&lt;/p&gt;

&lt;p&gt;1) You need to make the thing you want to do the default option, in some sense
2) You need to get yourself in a situation where you don’t have to go to a gigantic amount of effort each time in order to build a habit&lt;/p&gt;

&lt;p&gt;The book also suggests that you need some kind of reward mechanism for engaging in the habit you’re trying to build. I found that unnecessary, for reasons I’ll expand upon in a little bit.&lt;/p&gt;

&lt;h1 id=&quot;first-things-first&quot;&gt;First things first&lt;/h1&gt;

&lt;p&gt;How did I make it the default option? Well that’s easy, I just put the app icon for Duolingo in a really prominent place on my phone and decided that I would do it every day after brushing my teeth in the evening. So far so good. Thing is, this is designed to work really weell for a simple, bite-sized task that can be done based on a trigger like brushing your teeth.&lt;/p&gt;

&lt;p&gt;Even better, you can do it &lt;em&gt;while&lt;/em&gt; brushing your teeth. This is trickier though.&lt;/p&gt;

&lt;p&gt;What about a blog? Writing these posts actually takes a little time to think and plan aside from the writing itself, which isn’t necessarily quick either unless I’ve got something I really want to get off my chest. So, here’s where another trick in James Clear’s book helps.&lt;/p&gt;

&lt;p&gt;I’ve gotten into an extremely foolhardy bet with &lt;a href=&quot;https://lockwood.dev&quot;&gt;my friend Tom&lt;/a&gt;, which involves us both writing a blog post each week and seeing who can achieve greater reach, with the winner receiving a specified treat from our local café. Unfortunately given that he has suddenly turned into an opinion writer with no shortage of ideas I’m probably in a lot of trouble with regards to that bet. Fortunately, that’s not the point.&lt;/p&gt;

&lt;p&gt;What is the point is that now we have a goal to work towards. This alleviates part of the problem with turning something that isn’t bite-sized into a habit by giving a clear definition of whether or not the desired action has occurred. The problem now is one of how to make it reasonably automatic within that time frame.&lt;/p&gt;

&lt;h1 id=&quot;unproductively-wasting-time&quot;&gt;Unproductively wasting time&lt;/h1&gt;

&lt;p&gt;One problem I stumbled upon with building a Duolingo habit is that sometimes you’re just going through the motions. When you do the exercises every day, it’s inevitable that to some degree you rote learn the content. Don’t worry about breaking that just now, but instead figure out how to make sure you can learn something on top of it. It doesn’t even have to be much, but in any event you don’t want to come away from a Duolingo session having successfully avoided any mental exertion at all (which we are of course so good at avoiding).&lt;/p&gt;

&lt;p&gt;One thing you can do is make yourself say out loud either the question or answer from one question from each exercise. Could just be the first, to make it easy and get it over with. But that way you can’t do a non-zero amount of thinking in the worst case.&lt;/p&gt;

&lt;p&gt;Similarly with a blog. The challenge presents slightly differently. It’s very easy to want to write a blog post with little or no content in it, in order to just tick the box. So how do you make sure that &lt;em&gt;in the worst case&lt;/em&gt;, you haven’t just written a bunch of words? I don’t have a great answer for this yet, unfortunately, given that I’ve only been writing this blog for a little bit.&lt;/p&gt;

&lt;p&gt;There are a couple of things that point me in the direction of a solution, though. The first is to cut it down. &lt;a href=&quot;https://breakitdownto.earth/2019/06/06/Obfuscating_a_lack_of_AI.html&quot;&gt;My most widely-read post so far&lt;/a&gt; is actually perhaps the simplest one I’ve written. It’s very tempting to pack more and more thoughts into a post and wind up feeling like you’re going to too much effort. Instead just write the whole thing down as one sentence. Then turn that into a post.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Biting of more than you can chew is the fastest way to turn the exercise of writing a blog post into a box-ticking exercise, because you start minimising the effort on each part of the post and the whole suffers as a result.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It’s also worth taking the effort off yourself to remember a coherent argument when you sit down to write someting. So any time I have an idea for a post, I immediately create a draft with a single-sentence summary and a suggested publication date in the file name. That way, I have a few ideas I can work on and a way of prioritising which gets done. If you only write a bit of a post in one session, then that’s fine. By having a few being pushed forward at any one time, you don’t have to worry about finding the time for it. Now it’s possible to just write a paragraph of a post according to some schedule and the bigger picture takes care of itself.&lt;/p&gt;

&lt;h1 id=&quot;stuff-that-didnt-matter&quot;&gt;Stuff that didn’t matter&lt;/h1&gt;

&lt;p&gt;Part of the problem with setting out on writing a blog or learning Chinese is the lure of the goal is in a way self-defeating. We want to feel like we’ve accomplished the thing we’re setting out to achieve, but allowing yourself to contemplate that makes it harder because now you’re focusing on the amount of work you have to do &lt;em&gt;rather than just getting it all done&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;James Clear recommends rewarding yourself on successful completion of a task (or someone else in the case of my idiotic bet), but I find that you don’t need to emphasise this too much. Instead it can be easier to make the whole thing unthinking, in a way. Focussing on the goal or what you still want to achieve is demotivating in a way, I’ve found. Instead just try to make sure you get stuff done and that sort of thin has a way of magically taking care of itself.&lt;/p&gt;

&lt;p&gt;The downside is you don’t get the warm and fuzzy feeling involved in getting somewhere, instead it’s more of a mild surprise at saying something accurate in Chinese. Which is its own kind of fun, I guess.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Thanks Tom Roth for putting me onto it. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;I’m very aware that I’m not following this advice with this particular post, but that’s because I’m using this to clarify my own thoughts rather than say anything to anyone. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;This dovetails nicely with the &lt;em&gt;make it easy&lt;/em&gt; dictum from &lt;em&gt;Atomic Habits&lt;/em&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">I’ve been using Duolingo for about 470 days now to learn Chinese. There are a few things I’ve learned on the way, but mostly I’ve learned to apply the lessons of a book I read partway through this journey. In this post, I’ll talk about that and what it leads me to think about how to turn this blog into a durable habit.</summary></entry><entry><title type="html">How to gather a bunch of evidence and then ignore it</title><link href="http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence.html" rel="alternate" type="text/html" title="How to gather a bunch of evidence and then ignore it" /><published>2019-06-15T00:00:00+10:00</published><updated>2019-06-15T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence</id><content type="html" xml:base="http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence.html">&lt;p&gt;Oh my god this happens all the time I swear. If you’re working in machine learning then it is unavoidable that you work in the presence of uncertainty. This isn’t such a bad thing, but how we think as humans sometimes works against us when dealing with uncertainty. This post will look something that commonly goes wrong and more importantly, how to spot it so you can do something about it.&lt;/p&gt;

&lt;p&gt;Everyone’s different&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, so maybe this advice won’t work for everyone. But that’s fine.&lt;/p&gt;

&lt;h1 id=&quot;whats-the-story&quot;&gt;What’s the story?&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;

&lt;p&gt;When building a model it’s easy to get caught in a narrative fallacy. This was described in &lt;em&gt;The Black Swan&lt;/em&gt; by N. N. Taleb as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The problem is, this is really pervasive. And like any kind of absolutely normal background noise, it can sometimes to be hard to spot and focus on.&lt;/p&gt;

&lt;h1 id=&quot;thinking-is-hard-&quot;&gt;Thinking is hard :(&lt;/h1&gt;

&lt;p&gt;Fair enough. Machine learning models can be big complicated things, so knowing what’s going on in them is an active field of research in its own right, not just an onerous task for the practitioner. An easy way to catch yourself succumbing to a narrative fallacy is to look for yourself saying &lt;em&gt;“I know what’s going on here”&lt;/em&gt;, or words to that effect.&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This is because if you’re saying that you’re potentially focussing on the rare delight of being right, as opposed to sticking to the task of working out what you still don’t know. Try getting into the habit of talking about the state of your knowledge without referring to yourself and see how it changes your view.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I don’t think any less of anyone for thinking like this to be honest, large parts of an ML person’s job consist of being wrong. Wanting a win every now and then is only human. Problem is, a lot of the time when you think you’ve got it all sorted out, you’re actually shooting yourself in the foot at that very moment. It can really be a dirty, difficult job. So keep an eye on yourself if you find yourself desperate for a breakthrough, you might inadvertently join the dots in a way that allows you to hallucinate one.&lt;/p&gt;

&lt;h1 id=&quot;performance-statistic-hell&quot;&gt;Performance statistic hell&lt;/h1&gt;

&lt;p&gt;Turn it up to 11 if you find yourself deliberately glossing over any piece of evidence at your disposal because it conflicts with your understanding of your progress.&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; As a rule, I generally tend to track a number of different metrics when training a model and look for situations where their results conflict. Log loss going down but F1 going down with it? This incongruity is the sort of thing you want to know about, rather than disregarding one of these numbers because it doesn’t fit a proposed explanation about the functioning of your model. Go hunting for a mess in your performance statistics, just don’t over-interpret the evidence you have.&lt;/p&gt;

&lt;p&gt;A handy way to do that is to not let yourself get away from the definitions of the metrics you’re working with. Let’s work through the above example. In a classification setting, log loss is calculated as the logged distance between the prediction and label. So if that on average decreases, fine, predictions are closer to the actual labels &lt;em&gt;on average&lt;/em&gt;. The words “on average” are important because they ignore what your labels are - so shifts in log loss can be explained by an overall shift in the distribution of predictions created by your model. F1 gives you a blended measure of your model’s precision and recall. If F1 goes down, at least one of precision and recall went down. So you should inspect the confusion matrix to see what has changed. If it turns out that your predictions have become biased, it will appear here. If that’s not the case, then you can attach a lower probability to biased predictions being the cause of your results and instead look for outliers skewing your average loss result. This is a very different thought process to picking parts of the model and hypothesising about how they may have influenced your performance statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://breakitdownto.earth/2019/06/12/Automated_Mistakes_with_AutoML.html&quot;&gt;AutoML seems like a good way around all of this, but in my opinion it’s an iron law that in any situation a sufficiently innovative and eager person can always find a way to shoot themselves in the foot. You still need to be clear about what you’re up to.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;dont-believe-the-hype&quot;&gt;Don’t believe the hype&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;

&lt;p&gt;A textbook-definition narrative fallacy I see &lt;em&gt;everywhere&lt;/em&gt; consists of pretending your neural network functions the same way as a human brain and consequently believing that if you replicate your own decision-making process in constructing the network then all will be well. When it inevitably blows up temporarily or gives weird results - because it is in fact not a human brain - all of a sudden it’s very difficult to know what to do because that whole frame of reference constrains thinking. Stop doing this!&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; Of course there are plenty of very, very smart people who use the human brain analogy to educate others about how neural nets work, but that doesn’t mean you should consider it any kind of literal truth when building one yourself. Remember, that analogy is a simplification used for the purposes of education. No-one has an LSTM unit in their head.&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&quot;baby-steps&quot;&gt;Baby steps&lt;/h1&gt;

&lt;p&gt;One simple thing you can do to avoid falling for narrative fallacies is to slow down the process. I get it, your boss is an arsehole and you have to keep it moving. What I’m saying is that if you’re caught in a narrative fallacy, there can often be a strong urge to make multiple changes to your model at the same time, interpret whatever evidence you get as confirmation of the working hypothesis you had and then repeat this process. Again, this can be made worse by the rancid breath of your boss on your neck.&lt;/p&gt;

&lt;p&gt;Even worse, the real danger is that you wind up going around and around in circles without realising it due to a lack of clarity about the reasons for the effectiveness or otherwise of the changes you make, or in the worst case a lack of clarity about what you’ve even tried. Or you can wind up going round and round in circles, only to find a way out of it by means of another fallacious narrative that conveniently explains everything (but isn’t actually right). Occam’s Razor is pretty clear that sometimes the simple explanation can be the wrong one.&lt;/p&gt;

&lt;p&gt;Instead, force yourself to make only one change at a time. This does two things for you:&lt;/p&gt;

&lt;p&gt;1) It makes whether or not your change is a good one much clearer, and
2) It allows you to work out what to do next, because you now know precisely what to ascribe success or failure to.&lt;/p&gt;

&lt;p&gt;If you make three changes to a model, train it, then find the results are better - what was responsible? Like as not there’s nothing that really tells you, so the blanks get filled in subconsciously instead. Look at the functioning of your model at a lower level where possible and leave overarching narratives to themselves. Don’t leave room for the imagination by not trying to keep too much in your head at once and let it be an alarm bell if you find yourself really hankering to make a few changes at the same time. It can definitely feel like you’re moving a lot slower, but that’s because there’s a false economy in the alternative. Trying a scatter gun approach with a bunch of changes might feel like a quicker path to a result but not if you stop and consider all the time spent in that approach not being clear about what’s going on.&lt;/p&gt;

&lt;p&gt;This shouldn’t be news to anyone from a scientific background. But we aren’t all, are we? And even if you are, it’s important to make sure that these habits of mind are explicit, not tacit. It’s the unspoken parts of the train of thought that are at issue here.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;of course &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;…&lt;a href=&quot;https://www.youtube.com/watch?v=Wm54XyLwBAk&quot;&gt;morning glory&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;This isn’t meant to be snark. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;The reason for doing this is that &lt;em&gt;you are not your code&lt;/em&gt;. Keeping some mental distance from your code and models can help you think more freely about them. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Being able to spot yourself doing this and act is the hard part. In all seriousness, have a look at some cognitive behavioural therapy material. It’s not just for depressed people, it can help you work more effectively with your internal monologue and state of mind and not let this sort of thing slide through. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;Yup. Public Enemy jokes. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;If you’ve succeeded in building a model like this, well done. But are you sure you succeeded because you had the right approach? There might be cases where you explicitly do want to build a model to replicate some easily-understood process or object - was that so in your case, or is it the case that thinking that having an overarching narrative was the key to success is actually a narrative fallacy in its own right? That’s how pervasive this sort of thing can be. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Maybe Schmidhuber does. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Oh my god this happens all the time I swear. If you’re working in machine learning then it is unavoidable that you work in the presence of uncertainty. This isn’t such a bad thing, but how we think as humans sometimes works against us when dealing with uncertainty. This post will look something that commonly goes wrong and more importantly, how to spot it so you can do something about it.</summary></entry><entry><title type="html">How to automatically shoot yourself in the foot</title><link href="http://localhost:4000/2019/06/12/Automated_Mistakes_with_AutoML.html" rel="alternate" type="text/html" title="How to automatically shoot yourself in the foot" /><published>2019-06-12T00:00:00+10:00</published><updated>2019-06-12T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/12/Automated_Mistakes_with_AutoML</id><content type="html" xml:base="http://localhost:4000/2019/06/12/Automated_Mistakes_with_AutoML.html">&lt;p&gt;There are plenty of wonderful ways to screw up a perfectly good model. All of them share one common thread, which is that it’s always possible to shoot yourself in the foot. Even funnier (from my point of view at least), is that the urge to shoot oneself in the foot increases as the complexity of the model you’re working with does. In this post I’ll discuss one simple way of achieving a precision shot by means of automating a parameter search.&lt;/p&gt;

&lt;p&gt;To be clear, let’s define shooting oneself in the foot:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;“Shooting oneself in the foot consists of the ML practitioner evading complexity for the sake of mental, rather than computational, tractability. None of that stuff goes away because you ignore it lol.”&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;Stephen Hogg, 2019&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This partly but not completely explains a fair bit of the interest in automating the search for an acceptable neural network architecture. Automating something like that isn’t necessarily bad, but it can be if it’s just being used to avoid thinking about how models work.&lt;/p&gt;

&lt;h1 id=&quot;the-computer-just-does-it-for-you&quot;&gt;The computer just does it for you&lt;/h1&gt;

&lt;p&gt;Here’s a scenario that might lead you to want to partially automate things. You have a lot of text data and you need to discover topics in it. Obviously there are too many documents to read, so you use a model. One of the obvious candidates for doing this is Latent Dirichlet Allocation.&lt;/p&gt;

&lt;p&gt;Basically, LDA seeks to create a link between the words that are present in the collection (hereafter corpus), of documents you have and a number of latent (i.e. unobserved), topics. What this means in practice is that if you have an idea of how many topics there are in a corpus you can then get LDA to work out how they’re defined.&lt;/p&gt;

&lt;p&gt;The other piece of information LDA needs to work relates to the topics themselves. Specifically, how narrowly defined are they? Do you only need one or two words to establish the presence of a topic in a document, or more? Now how do we tell which values for our parameters are good? With a model like LDA, typically you’d evaluate the &lt;em&gt;per word perplexity&lt;/em&gt;. That’s a mouthful, so let’s start with the last word.&lt;/p&gt;

&lt;h1 id=&quot;confused-yet&quot;&gt;Confused yet?&lt;/h1&gt;

&lt;p&gt;Imagine you’re playing cards with someone. Normal deck of 52 as is common in the West, nothing too funny going on. Your opponent draws a card unseen to you. What could it be?&lt;/p&gt;

&lt;p&gt;If you don’t know anything at all and have to make a wild guess, then there are 52 potential cards you could guess from. You have terrible odds of being right. In this case, your perplexity is 52 - the number of states you need to choose from.&lt;/p&gt;

&lt;p&gt;If, on the other hand, you know what cards are in your own hand or have seen others previously, then you can eliminate them as possibilities. If you can eliminate, say 6 cards, then that leaves 46 possible cards your opponent may have drawn. So perplexity is now 46 and you’re still guessing, but with a bit more clarity. The idea with modelling is to drive that number even lower by having your model eliminate outcomes that aren’t really plausible. In this case it might be that your opponent’s behaviour indicates the presence or absence of a certain card - what this boils down to is using the data you have to narrow the scope of outcomes you think are possible. Which is a great way of describing what a model does.&lt;/p&gt;

&lt;h1 id=&quot;this-all-seems-fine&quot;&gt;This all seems fine&lt;/h1&gt;

&lt;p&gt;Going back to LDA (this is the &lt;em&gt;per word&lt;/em&gt; part of &lt;em&gt;per word perplexity&lt;/em&gt;), what gets measured is how strongly each word in the vocabulary of your corpus is associated to any one topic. If you have no idea which topic a word relates to, then perplexity will be equal to the number of topics. Equally, if a given word clearly relates to only one topic then perplexity drops to one. You’ll get a range of outcomes for the words you’re working with, so it just gets averaged and called &lt;em&gt;per word perplexity&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So we have a couple of levers and a way of telling whether words are getting more or less tightly associated with topics. More is better, generally. So now we can rig up something to try a few different combinations of values for the number of topics and how narrowly defined they are, check the perplexity and pick whatever has the lowest score.&lt;/p&gt;

&lt;p&gt;And that’s where things go wrong. Back when I used to work in a call centre, people used to constantly say that what gets measured, gets managed. So if all you worry about is the uniqueness of the relationship between words and topics, that’s what you’ll get!&lt;/p&gt;

&lt;p&gt;It turns out that if you try to optimise both the number of topics and their narrowness at the same time, you wind up in either of two trivial cases.&lt;/p&gt;

&lt;p&gt;1) One MASSIVE topic, strongly connected to every single word so that every document has the same primary topic&lt;/p&gt;

&lt;p&gt;2) One topic per word, so that you haven’t really clustered anything at all and are really just calculating term frequency without realising&lt;/p&gt;

&lt;p&gt;Both of these outcomes are rubbish, obviously. It’s worth noting that they could also appear by stealth: a configuration with 30 topics but all the words are associated to only one of them could happen, for example. You’d then inadvertently pick it as the optimal configuration and think everything is hunky dory because the parameters that led to that scenario don’t seem untoward.&lt;/p&gt;

&lt;h1 id=&quot;time-to-eat-your-vegetables&quot;&gt;Time to eat your vegetables&lt;/h1&gt;

&lt;p&gt;The warning that this was going to fail happened in the word “generally”, in the previous paragraph. How precisely did this stuff up, though?&lt;/p&gt;

&lt;p&gt;Firstly, doing things this way basically consisted of trying to abstract away thinking about how the model works. Secondly, the phrase &lt;em&gt;what gets measured, gets managed&lt;/em&gt; really matters here. When automatically optimising something, first take a second to think. If you managed to get that score all the way down (or up), would that actually be the outcome you wanted? A lot of the time the answer is yes, but it’s the times when the answer is really no that will get you. Just a little bit of thinking saves a lot of work.&lt;/p&gt;

&lt;p&gt;Another thing to think about is how ambitious you get about automatically optimising stuff. If you decided to have an opinion about the narrowness of topics, then your task would simply be a matter of employing something like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Elbow_method_(clustering)&quot;&gt;Elbow method&lt;/a&gt; to make a choice about the right number of topics. This is much less fraught because you now have some guardrails for the process. The same in reverse applies to fixing the number of topics and searching over the narrowness of their definition.&lt;/p&gt;

&lt;p&gt;Finally, note that I have not recommended making the metric you optimise more complicated to account for some of these quirks. Down that road lies madness, but go ahead if you really want. Good luck with that haha.&lt;/p&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">There are plenty of wonderful ways to screw up a perfectly good model. All of them share one common thread, which is that it’s always possible to shoot yourself in the foot. Even funnier (from my point of view at least), is that the urge to shoot oneself in the foot increases as the complexity of the model you’re working with does. In this post I’ll discuss one simple way of achieving a precision shot by means of automating a parameter search.</summary></entry><entry><title type="html">How to obfuscate the fact that you’re not really doing AI</title><link href="http://localhost:4000/2019/06/06/Obfuscating_a_lack_of_AI.html" rel="alternate" type="text/html" title="How to obfuscate the fact that you're not really doing AI" /><published>2019-06-06T00:00:00+10:00</published><updated>2019-06-06T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/06/Obfuscating_a_lack_of_AI</id><content type="html" xml:base="http://localhost:4000/2019/06/06/Obfuscating_a_lack_of_AI.html">&lt;p&gt;&lt;img src=&quot;/assets/img/D07CcPPXcAAKU5L.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh my god this happens all the time I swear. What I’m talking about here is the thing where you build a model and think it works, but if you look closer it’s actually doing almost nothing. This post looks at a common yet (in my opinion) not widely discussed trap you (or better yet, someone else), can fall into.&lt;/p&gt;

&lt;h2 id=&quot;even-simple-models-can-stuff-up&quot;&gt;Even simple models can stuff up&lt;/h2&gt;

&lt;p&gt;Once upon a time I saw a logit model that was built to predict customer churn. The relevant group in charge of it had performance stats to back up their assertion that it worked well. But a quick glance suggested all was not as it seemed.&lt;/p&gt;

&lt;p&gt;What gave this away? It’s pretty simple, in fact. The variables of the model were all pretty straightforward - indicators for variables, some customer history variables and some other stuff to handle the frequency of their interactions with the company. So far so good. The problem with all this becomes apparent when you look at the coefficients of the model. Almost all of them were near zero, except for the dummy variable for one product which was massive.&lt;/p&gt;

&lt;p&gt;In other words, the model was just one giant &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt; statement. If you had this product, you would churn, otherwise you wouldn’t. The reason this model seemed like it was ok was because customers with that particular product were indeed churning &lt;em&gt;en masse&lt;/em&gt;, such that it masked performance flaws elsewhere in the sample space.&lt;/p&gt;

&lt;p&gt;In the course of a few months’ worth of trying to alert people to this, I realised that to accomplish that aim you need a good explanation for how that could have happened in the first place. This brings us to the villain of this particular story, a thing called &lt;em&gt;quasi-separation&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;separation-anxiety&quot;&gt;Separation anxiety&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/&quot;&gt;This page&lt;/a&gt; gives as good an explanation as any I’ve seen as to what’s going on here. Long story short, the dataset used to build the model has a severe bias in it, such that it’s possible to make predictions about the label with 100% confidence in some circumstances.&lt;/p&gt;

&lt;p&gt;In the case I described above, what had happened was that the model was trained on a dataset that &lt;em&gt;contained no examples of a customer having that one product and also not churning&lt;/em&gt;. So maximum likelihood estimation takes that and runs with it and you wind up with a model that’s ridiculously oversensitive to this one variable. The confusion matrix shows this more clearly:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;churn&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;no churn&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Has product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;125&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Doesn’t have product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Any time you see a sample with the &lt;strong&gt;HAS_PRODUCT&lt;/strong&gt; flag, why wouldn’t you predict they churn? It’s a one-way bet.&lt;/p&gt;

&lt;p&gt;This is a really clear-cut case, unfortunately this can still happen in slightly less obvious circumstances. In the link at the start of this section, they talk about a case of exact separation with a continuous instead of binary variable. Similarly, even if the 0 in that confusion matrix was a 2 or a 5 you’d still have the problem. This can also kick in if there’s a combination of variables that exactly splits your labels.&lt;/p&gt;

&lt;p&gt;The most obvious knee-jerk reaction is to regularise your model, but that’s a band-aid. Really the data is the problem. And potentially the fact that you’re using a maximum likelihood technique. But that’s a matter for a separate rant some other time.&lt;/p&gt;

&lt;p&gt;Now it’s time for a quick FAQ on what we’ve learned!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q: My neural network doesn’t have that problem&lt;/p&gt;

  &lt;p&gt;A: Yes it does. If you’ve got separation problems in your data, then you’re still stuffed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;how-would-i-know&quot;&gt;How would I know?&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;The obvious thing is to run that confusion matrix for every categorical variable you’ve got and look for any that have a cell with barely any observations as a percentage of the total. You might also consider looking at a covariance matrix or two. The other thing you can do is just run your model and look for the signs that it has gone wrong. Remember, there are a few giveaways.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;One of the coefficients is orders of magnitude bigger than the others and the standard errors are massive because the model hasn’t really converged&lt;/li&gt;
  &lt;li&gt;Some of your predictions are almost exactly zero or one - there should almost never be a reason for a model to be THAT confident&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is almost surely going to happen to you at some point, if it hasn’t already. Good luck with that haha.&lt;/p&gt;

&lt;p&gt;I bet at least a few start-ups have got models that are really doing this under the hood.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Ok fine, this is a Soundgarden joke. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/D07CcPPXcAAKU5L.png" /></entry><entry><title type="html">Two Cheers for StanfordNLP!</title><link href="http://localhost:4000/2019/05/25/Two_Cheers_for_Stanfordnlp.html" rel="alternate" type="text/html" title="Two Cheers for StanfordNLP!" /><published>2019-05-25T00:00:00+10:00</published><updated>2019-05-25T00:00:00+10:00</updated><id>http://localhost:4000/2019/05/25/Two_Cheers_for_Stanfordnlp</id><content type="html" xml:base="http://localhost:4000/2019/05/25/Two_Cheers_for_Stanfordnlp.html">&lt;p&gt;As you may or may not know, the &lt;a href=&quot;https://github.com/UniversalDependencies&quot;&gt;Universal Dependencies Github repo&lt;/a&gt; is an absolutely wonderful source of tagged NLP data that can be used for a wide variety problems. It’s worth reinforcing, it’s a great resource and an example to us all about where academic collaboration can lead.&lt;/p&gt;

&lt;p&gt;So it’s nice to see people doing stuff with it. In particular, this means the latest work done in PyTorch by the &lt;a href=&quot;https://stanfordnlp.github.io/stanfordnlp/&quot;&gt;Stanford NLP group&lt;/a&gt;. Have a look around and you’ll see a very large range of pretrained models put together to solve quite a few different tasks. Look at the system performance page and you’ll see that they all work pretty well, too.&lt;/p&gt;

&lt;p&gt;What’s the drawback, then?&lt;/p&gt;

&lt;h2 id=&quot;what-if-we-tried-something-simpler&quot;&gt;What if we tried something simpler?&lt;/h2&gt;

&lt;p&gt;One reasonable thing you might ask yourself is - how much better do these models perform than a much simpler version that accomplishes the same task? It’s actually pretty easy to get an answer to this, it turns out.&lt;/p&gt;

&lt;p&gt;One of the prior works of the Stanford guys was &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP&quot;&gt;CoreNLP&lt;/a&gt;, which was originally written in Java and didn’t use any kind of neural network at all. Instead, it used [Conditional Random Fields], which are a kind of probabilistic graph. Leaving almost all of the differences between a CRF and the LSTM-based neural network approach aside, take it as read that the models you can use in CoreNLP are a lot, lot simpler.&lt;/p&gt;

&lt;p&gt;All we really need to do is get the underlying data that was used to generate the PyTorch models and chuck it into CoreNLP and see what the results tell us.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results!&lt;/h2&gt;

&lt;p&gt;For the purposes of this discussion, let’s focus on POS (part of speech) tagging. There are two reasons for this:
1) That’s why I was doing this in the first place, and
2) It’s a pretty common problem&lt;/p&gt;

&lt;p&gt;Here’s a small table with accuracy stats for a few corpora from the Universal Dependencies repo.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Corpus&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;PyTorch&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;CoreNLP&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;UD_Vietnamese-VTB&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;79.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;87.7&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;UD_Indonesian-GSD&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;93.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;96.6&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The more interesting thing than the details of these particular models is that they are outperforming the LSTM-based approach. That is, they’re both (heaps) faster and more accurate for this task using this dataset. Yes, this approach doesn’t consider every possible language, but hopefully with a few languages the point is already coming across.&lt;/p&gt;

&lt;p&gt;So now what?&lt;/p&gt;

&lt;h2 id=&quot;two-cheers&quot;&gt;Two cheers&lt;/h2&gt;

&lt;p&gt;This post isn’t meant as an attempt at knocking StanfordNLP or anyone associated. Very far from it. The Stanford NLP group did us all a favour by putting these models together, not just because it gives you something to work with but also because in order to make progress in this field we actually have to try stuff out at some stage. Leading by doing and actually putting together a bunch of models on the back of UD data is a great way to encourage the usage of that dataset. They’re also owed a debt of gratitude for having done CoreNLP before that, which I personally use and find extremely handy.&lt;/p&gt;

&lt;p&gt;Instead, the point of this exercise was to suggest that some of the problems we face in the field of machine learning aren’t going away. Bigger isn’t necessarily better when it comes to model complexity and any result needs to be viewed in context of a reasonable baseline. A lot of machine learning is a long, slow plod that you kind of just have to go through. But that doesn’t necessarily mean any part of it isn’t worth it.&lt;/p&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">As you may or may not know, the Universal Dependencies Github repo is an absolutely wonderful source of tagged NLP data that can be used for a wide variety problems. It’s worth reinforcing, it’s a great resource and an example to us all about where academic collaboration can lead.</summary></entry><entry><title type="html">Transformers: more than meets the eye? (Part 1: The Past)</title><link href="http://localhost:4000/2019/05/10/Transformers_part1.html" rel="alternate" type="text/html" title="Transformers: more than meets the eye? (Part 1: The Past)" /><published>2019-05-10T00:00:00+10:00</published><updated>2019-05-10T00:00:00+10:00</updated><id>http://localhost:4000/2019/05/10/Transformers_part1</id><content type="html" xml:base="http://localhost:4000/2019/05/10/Transformers_part1.html">&lt;p&gt;There’s quite a lot of talk about how wonderful Transformers are and how they solve a lot of problems. Fair enough, they are pretty good. That said, all the explanations I’ve seen so far include math and neural network diagrams (nothing wrong with that!). What I’m going to attempt to do is talk about how they work in a more conceptual way, because it’s much easier to understand the math and diagrams if you have some idea of why those equations even exist.&lt;/p&gt;

&lt;p&gt;In this post I’ll attempt to walk through the history that got us to the point where Transformers (the neural network technique, for clarity), seemed like a good idea. There are a couple of things we’ll need to think about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The situation we use a Transformer in - and why&lt;/li&gt;
  &lt;li&gt;How have we evolved in thinking about how to deal with natural language?&lt;/li&gt;
  &lt;li&gt;Why wasn’t any of the stuff we thought of in the past good enough? Why did we have to keep going and invent transformers?&lt;/li&gt;
  &lt;li&gt;How do transformers work?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll look at the first two points in this post and the second two in the next post. I’ll be assuming you know what you’re doing with neural networks, to at least some extent.&lt;/p&gt;

&lt;p&gt;Hopefully what you’ll see at the end of all of this is that Transformers contain echoes of all the things that came before and that in conceptual terms they’re actually quite simple. Finally, I’m going to talk about this through the lens of Natural Language Processing (NLP), because that’s what these things get used for. If you’re interested in this with respect to financial time series stuff or otherwise, just squint really hard and imagine some of the words are replaced with whatever the appropriate domain-specific replacement is.&lt;/p&gt;

&lt;h2 id=&quot;what-problem-is-a-transformer-meant-to-solve&quot;&gt;What problem is a transformer meant to solve?&lt;/h2&gt;

&lt;p&gt;This is actually the simple bit of these posts, if anything is. It’s what is known as  a &lt;em&gt;sequence to sequence&lt;/em&gt; problem. Also known as a &lt;em&gt;seq2seq&lt;/em&gt; problem if you’re one of the people who works on this sort of thing.&lt;/p&gt;

&lt;p&gt;Now, what the heck is that?&lt;/p&gt;

&lt;p&gt;Basically, it’s the sort of thing you see over and over in NLP. Translation is a perfect example. Take for instance the following input and output pair:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Input : I do want to eat vegetables
Output: 我想吃蔬菜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal of this task is to turn the input (English), into the output (Mandarin). Clearly, the input is just a bunch of words and so is the output. BUT! The order in which they appear matters. To see this in action, the input “do I want to eat vegetables” clearly has a different meaning. Some languages might have less of a problem with this than others, but in any event it’s a real source of information that we want to incorporate when we’re working on this problem. So the challenge is, how to predict a sequence of values - not necessarily words, it could be a financial time series problem you’re working on - using not just the knowledge of the input values but their structure (i.e. order), as well.&lt;/p&gt;

&lt;p&gt;Just to briefly emphasise, this general type of problem has existed forever. People creating neural networks didn’t invent this type of problem, only new ways of solving it.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;important-digression&quot;&gt;Important digression!&lt;/h3&gt;
&lt;p&gt;Looking at the above example and a great deal of explanatory material on the topic of &lt;em&gt;seq2seq&lt;/em&gt; problems, there’s a common problem you can encounter. It’s this: the problem looks like the input and output are concurrent, but they actually aren’t.&lt;/p&gt;

&lt;p&gt;The way the problem works is, you feed in an input sequence and then want your model to complete the sequence. So really, the input and output pair above should be presented more like this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I do want to eat vegetables &lt;strong&gt;BREAK&lt;/strong&gt; 我想吃蔬菜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The reason for the &lt;strong&gt;BREAK&lt;/strong&gt; there is that you need something to tell your model to stop reading input and start generating output.&lt;/p&gt;

&lt;p&gt;This will become very important as time goes on in this discussion.&lt;/p&gt;

&lt;h2 id=&quot;baby-steps&quot;&gt;Baby steps&lt;/h2&gt;
&lt;p&gt;Ok so first let’s step back in time.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; In the beginning (and I mean &lt;em&gt;actually&lt;/em&gt; a long time ago, like 40 years or more), we didn’t have any of the gleaming mathematical machinery we now do for working on language problems. So where to start?&lt;/p&gt;

&lt;h3 id=&quot;n-grams&quot;&gt;N-grams&lt;/h3&gt;
&lt;p&gt;This was the first, and simplest go at trying to figure out the &lt;em&gt;what comes next?&lt;/em&gt; problem beyond just feeding in your input and hoping for the best. The idea here is that you can actually do a better job of predicting the next word if you use not only the current one, but the one before it. All the “N” in “N-grams” tells you is that you might use more or less context as opposed to specifically one or two words of history. The following examples might make things clearer.&lt;/p&gt;

&lt;p&gt;Guess the next word given the input:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is basically impossible, right? There’s some stuff that might not be that likely to come next, but by and large we can’t really narrow it down.&lt;/p&gt;

&lt;p&gt;What about this input:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;weather is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a bit more workable. We could probably make a few guesses about what goes next, but it’s definitely a narrower scope of possibilities than last time.&lt;/p&gt;

&lt;p&gt;Now what about this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The weather is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point I’m fairly sure something like &lt;em&gt;hot&lt;/em&gt; or &lt;em&gt;cold&lt;/em&gt; comes next.&lt;/p&gt;

&lt;p&gt;So far so good, but what’s the cost? It’s pretty simple, in fact. Firstly, the reason we know one of those words comes next is because we’ve all watched too much news on the TV so far in this life. That’s well and good for a human, but sometimes you don’t have enough data for that to work for a machine.&lt;/p&gt;

&lt;p&gt;Another problem arises if you’re trying to predict more than just the next word. If you’re trying to predict a whole sentence, you’re in a lot of trouble. Maybe you can get the first word of your output correct, but after that you’re at the mercy of whatever you predicted as your first word. If that’s wrong, everything else becomes even wronger. We’ve only really started solving this problem comparatively recently.&lt;/p&gt;

&lt;p&gt;Even worse, the longer your input sequence the rarer it is! In the case of “The weather is”, you might be able to see it a few times in a dataset if you’re lucky. But not as many times as you see “weather is”, perhaps. So we’re stuck in a classic problem where we can get a few edge cases right all of the time, or have hopefully better than random performance over a much broader set of cases, assuming your input doesn’t fundamentally change as time goes on. That’s probably not good enough. So people kept researching stuff.&lt;/p&gt;

&lt;h3 id=&quot;time-becomes-a-loop&quot;&gt;Time becomes a loop&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Another thing that people discovered when talking about &lt;em&gt;seq2seq&lt;/em&gt; problems is that sometimes stuff a long time ago in your input sequence actually matters in terms of the prediction you make at the end of it. Doing stuff with N-grams won’t work here because for example a 128-gram is basically unique. You’ll only every see that sequence of tokens the once, if at all, so it’s no use for prediction. So what can you do about this?&lt;/p&gt;

&lt;p&gt;This is where things start getting complicated, unfortunately. That is to say, as soon as you try and do anything beyond the most basic stuff with a &lt;em&gt;seq2seq&lt;/em&gt; problem, it gets complicated.&lt;/p&gt;

&lt;p&gt;Intuitively, what we want is something that can remember the earlier parts of a sequence like this:&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I was born in a water moon. Some people, especially its inhabitants, called it a planet, but as it was only a little over two hundred kilometres in diameter, ‘moon’ seems the more accurate term. The moon was made entirely of water, by which I mean it was a globe that not only had no land, but no rock either, a sphere with no solid core at all, just liquid water, all the way down to the very centre of the globe.&lt;/p&gt;

  &lt;p&gt;If it had been much bigger the moon would have had a core of ice, for water, though supposedly incompressible, is not entirely so, and will change under extremes of pressure to become ice. (If you are used to living on a planet where ice floats on the surface of water, this seems odd and even wrong, but nevertheless it is the case.) The moon was not quite of a size for an ice core to form, and therefore one could, if one was sufficiently hardy, and adequately proof against the water pressure, make one’s way down, through the increasing weight of water above, to the very centre of the moon.&lt;/p&gt;

  &lt;p&gt;Where a strange thing happened.&lt;/p&gt;

  &lt;p&gt;For here, at the very centre of this watery globe, there seemed to be no gravity. There was colossal pressure, certainly, pressing in from every side, but one was in effect weightless (on the outside of a planet, moon or other body, watery or not, one is always being pulled towards its centre; once at its centre one is being pulled equally in all directions), and indeed the pressure around one was, for the same reason, not quite as great as one might have expected it to be, given the mass of water that the moon was made up from.&lt;/p&gt;

  &lt;p&gt;This was, of course,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Imagine you’re trying to guess what comes next after “This was, of course,”. Do you think it’s something about the physics stuff towards the end of the passage, or about the perspective of people who live on planets where ice floats on the surface referred to much earlier in the text? It could be either, but if your model isn’t capable of having some way of hanging onto relevant info from much earlier in your input sequence, you have no choice. You can only really consider the physics option.&lt;/p&gt;

&lt;p&gt;So we needed some way of hanging onto not everything but at least some important info from the earlier parts of a potentially long sequence. This is where neural networks start to play a role.&lt;/p&gt;

&lt;p&gt;The original go at this involved trying to get a neural network to have a memory of its own, after a fashion.&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; I don’t want to go on forever about this because it turns into talking about math, but the long and the short of it was that you wanted a bit of your network to loop around on itself in a way that made some of the information coming in from a given sequence element take a while to get back out again.&lt;/p&gt;

&lt;p&gt;It’s a bit like Gumbo. &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; This is a slightly weird analogy, but stick with me. Gumbo is this dish you get in the southern parts of the U.S. that takes &lt;em&gt;forever&lt;/em&gt; to cook. But the trick is to make it so that the flavour of the ingredients you put in early on is still retained some hours later in the pot when you finally finish cooking, even if you’ve taken that ingredient back out.&lt;/p&gt;

&lt;p&gt;In our case, we want to put words in and move on to subsequent parts of the sequence without necessarily losing all of the earlier flavour. By the time we get to the end of that passage I put in above, we still want to be able to have some hint of the earlier part where the speaker says his planet was a moon.&lt;/p&gt;

&lt;p&gt;In practical terms, what you do is make information take a really long path to get out of your network. Problem is, this isn’t perfect either. For a really long sequence, you can wind up with a gigantic number of parameters in your network. You also don’t really know what to hang on to, so by creating a recurrent neural network all you’re really doing is trying to hang on to as much of your input information as possible for as long as possible. This is on top of the other problem where introducing recurrence makes it slower to process an input sequence because now you have a dependency problem: you can only process some of your input after you’ve processed the bits that came before.&lt;/p&gt;

&lt;p&gt;That said, it’s still a big big step forward.&lt;/p&gt;

&lt;h3 id=&quot;my-model-accurately-predicts-everything&quot;&gt;My model accurately predicts everything!&lt;/h3&gt;
&lt;p&gt;The next step forward after recurrence seemed at first like it solved everything. We’ll see that wasn’t the case though.&lt;/p&gt;

&lt;p&gt;It turns out two of the problems with the basic approach to recurrence can be solved in the same way. The non-selectiveness of memory storage in a basic recurrent net and the escalating number of parameters needed were both addressed by the Long Short-Term Memory cell.&lt;/p&gt;

&lt;p&gt;What this thing did differently was conceptually simple enough. Rather than just adding links between cells in a neural network so that it took a while for information to find the exit, each cell now got its own memory storage.&lt;/p&gt;

&lt;p&gt;As always, so far so good. With this particular invention, you could get to the end of a passage like the one in the previous section and have some sort of useful numerical information from earlier on predict what comes next.&lt;/p&gt;

&lt;p&gt;Going back to the first example I gave:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I do want to eat vegetables &lt;strong&gt;BREAK&lt;/strong&gt; 我想吃蔬菜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that we have an LSTM to work with, the input sequence can comfortably be quite large and we’ll still have all of it to work with when we start predicting an output sequence.&lt;/p&gt;

&lt;p&gt;That’s another big step forward, but there’s still a problem here. Now, although we have a lot of information to work with when we start predicting an output sequence, we still have the problem that the sequence is only as good as the first prediction.&lt;/p&gt;

&lt;p&gt;That is to say, we need the entire sequence to be meaningfully encoded in whatever you have after you’ve looked at the last item in your input. It also has to be able to let you spell out the entire output sequence. And when you do, you’re still stuck with the problem that your second prediction depends on your first one and your third prediction depends on your second one.&lt;/p&gt;

&lt;p&gt;Also this LSTM stuff is relatively slow because you have to process one element of your sequence at a time and there’s no real way around that in the framework. That said, as always, it was a big leap forwards and started to show some serious potential for dealing with &lt;em&gt;seq2seq&lt;/em&gt; problems.&lt;/p&gt;

&lt;p&gt;Wouldn’t it make more sense and potentially be faster if we could use all of the bits of the input sequence independently in determining an output, instead of pancaking the information you have up against an invisible wall separating your input sequence and whatever comes next? This question is more or less what the Attention method and Transformers grow out of. I’ll talk about why next time.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Just in case anyone was wondering. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;This is secretly a joke. If you work out what I’m referring to, I’m not sorry. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DNb4VKln1uw&quot;&gt;https://www.youtube.com/watch?v=DNb4VKln1uw&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;From &lt;em&gt;The Algebraist&lt;/em&gt; by Iain M. Banks. R.I.P. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Not as cool as anything invented by Miles Bennett Dyson, but still. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gumbo#Preparation_and_serving&quot;&gt;https://en.wikipedia.org/wiki/Gumbo#Preparation_and_serving&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">There’s quite a lot of talk about how wonderful Transformers are and how they solve a lot of problems. Fair enough, they are pretty good. That said, all the explanations I’ve seen so far include math and neural network diagrams (nothing wrong with that!). What I’m going to attempt to do is talk about how they work in a more conceptual way, because it’s much easier to understand the math and diagrams if you have some idea of why those equations even exist.</summary></entry></feed>