<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-30T16:52:41+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">It definitely worked on my computer</title><subtitle>This blog contains wild-eyed rants and  things that I need to write down somewhere.  Obviously, the views expressed here are my own.
</subtitle><author><name>Stephen Hogg</name></author><entry><title type="html">Qualitative Assessment is a Nightmare</title><link href="http://localhost:4000/2019/08/30/Assessing_Subjective_Quality.html" rel="alternate" type="text/html" title="Qualitative Assessment is a Nightmare" /><published>2019-08-30T00:00:00+10:00</published><updated>2019-08-30T00:00:00+10:00</updated><id>http://localhost:4000/2019/08/30/Assessing_Subjective_Quality</id><content type="html" xml:base="http://localhost:4000/2019/08/30/Assessing_Subjective_Quality.html">&lt;p&gt;Last week I had an idea about maybe using FaceNet or some other Siamese network to create word embeddings. Turns out &lt;a href=&quot;https://www.researchgate.net/publication/321302050_Beyond_Word2Vec_Embedding_Words_and_Phrases_in_Same_Vector_Space&quot;&gt;plenty of people&lt;/a&gt; &lt;a href=&quot;https://chauff.github.io/dir2016/accepted-submissions/kenter.pdf&quot;&gt;have had&lt;/a&gt; &lt;a href=&quot;https://github.com/dhwajraj/deep-siamese-text-similarity&quot;&gt;that idea&lt;/a&gt;, or some variant thereof. So doing it myself looks a bit silly.&lt;/p&gt;

&lt;p&gt;There’s still something to think about here: how does one decide whether word vectors are good or not? The obvious way to look at this is to ask yourself whether they let you do the task you need them for. For someone wanting to make a library of pre-trained vectors available (such as those of &lt;a href=&quot;https://fasttext.cc/docs/en/crawl-vectors.html&quot;&gt;fastText&lt;/a&gt;, for example), it isn’t as simple. You might try comparing two libraries of word vectors across a number of tasks, or maybe even doing the old &lt;a href=&quot;https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html&quot;&gt;word analogy trick&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More broadly, how do you do something about assessing the &lt;em&gt;quality&lt;/em&gt; of a machine learning solution when reasonable people can differ on the answer?&lt;/p&gt;

&lt;h1 id=&quot;sometimes-its-easiest-to-bury-your-head-in-the-sand&quot;&gt;Sometimes it’s easiest to bury your head in the sand&lt;/h1&gt;

&lt;p&gt;We know how to measure model performance in objective terms, but if you’re interested in something subjective it’s easy to waste your time learning nothing. The problem with subjectively assessing the output of a machine learning model is precisely that.&lt;/p&gt;

&lt;p&gt;The bog standard thing people do in this situation (in my experience), is decide to look at a sample of data manually and get a human to make a definitive judgement about whether the model performed well or not. I prefer to think of this as “pretending there isn’t a problem”. By getting a human to make a definitive judgement, you’re implicitly saying that the person involved is always right and discounting the possibility of other viewpoints. The common (in my experience) answer to this problem is also naïve. Getting a few humans to check and agree on how they will assess things doesn’t solve anything. When people inevitably disagree, who is right?&lt;/p&gt;

&lt;p&gt;The even bigger problem here is that inevitably, when you’re verifying things manually it’s hard to check more than a little data. This is &lt;em&gt;definitionally&lt;/em&gt; the most labourious approach for verifying the performance of a model!&lt;/p&gt;

&lt;h1 id=&quot;well-that-didnt-work&quot;&gt;Well that didn’t work&lt;/h1&gt;

&lt;p&gt;At this stage, it’s natural to start looking for alternatives. Rather than looking for definitive proof that a model works (or doesn’t), instead it’s better to just look for &lt;em&gt;evidence&lt;/em&gt;. This is where the word analogy thing comes in again. It’s a perfect example of what to do, in fact. Start with an initial idea of something that should be desirable in the outputs you have and then check whether that’s the case. Then come up with something else that intuition suggests should emerge from the results you have generated. Test it and move on. Then it’s just a matter of keeping going until you’ve got enough bits of evidence to feel like they add up to something.&lt;/p&gt;

&lt;p&gt;One of the reasons the word analogy trick is such a great example is that &lt;a href=&quot;https://books.google.com.au/books?id=_LBmDwAAQBAJ&amp;amp;pg=PA40&amp;amp;lpg=PA40&amp;amp;dq=fasttext+king+queen&amp;amp;source=bl&amp;amp;ots=nl1uX3NRpR&amp;amp;sig=ACfU3U32Y2i_j5fyoFDnKtwPt13r1271UQ&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=2ahUKEwiUoIC5rafkAhXp7HMBHSzRCtUQ6AEwGXoECAkQAQ#v=onepage&amp;amp;q=fasttext%20king%20queen&amp;amp;f=false&quot;&gt;sometimes it shows you bad results&lt;/a&gt;. If that happens, it’s a simple matter to decide a) whether that matters to you or not and b) how it colours your view of whether this model works or not.&lt;/p&gt;

&lt;p&gt;I find this approach works better for a few reason. The first is that you wind up assessing a lot more data, because it’s a much less labourious approach. That in turn should reduce the scope for nasty surprises down the track. The second is that this explicitly accepts the subjective nature of the problem. By not looking for binary, definitive proof and instead just amassing indications, the fact that different people can have different views isn’t necessarily as big of a deal.&lt;/p&gt;

&lt;p&gt;I’ll be gone for the next little bit but should be back at the end of September!&lt;/p&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Last week I had an idea about maybe using FaceNet or some other Siamese network to create word embeddings. Turns out plenty of people have had that idea, or some variant thereof. So doing it myself looks a bit silly.</summary></entry><entry><title type="html">A random thought about FaceNet</title><link href="http://localhost:4000/2019/08/24/Random_thought_about_FaceNet.html" rel="alternate" type="text/html" title="A random thought about FaceNet" /><published>2019-08-24T00:00:00+10:00</published><updated>2019-08-24T00:00:00+10:00</updated><id>http://localhost:4000/2019/08/24/Random_thought_about_FaceNet</id><content type="html" xml:base="http://localhost:4000/2019/08/24/Random_thought_about_FaceNet.html">&lt;p&gt;The problem of facial recognition is now pretty much solved thanks to &lt;a href=&quot;https://arxiv.org/abs/1503.03832&quot;&gt;FaceNet&lt;/a&gt;.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The underpinning insight that makes it all
come together is analogous to the way an SVM works. But have we done all we can with this rather novel form of network?&lt;/p&gt;

&lt;p&gt;Rather than feeding in one example at a time with a corresponding label, in the FaceNet architecture three examples are presented with the intention that
&lt;a href=&quot;https://www.youtube.com/watch?v=rsRjQDrDnY8&quot;&gt;one of these things is not like the others&lt;/a&gt;. The loss function just tries to make sure that the ensuing model respects that fact for a 
given triplet of examples, similar to how an SVM just tries to maximise the margin between classes rather than accurately predict the value for every single observation.&lt;/p&gt;

&lt;h1 id=&quot;now-to-try-and-do-something-vaguely-interesting&quot;&gt;Now to try and do something vaguely interesting&lt;/h1&gt;

&lt;p&gt;As someone working in NLP, I immediately wondered about the possibility of using this technique to encode word vectors. Not only that, but whether they would be worthwhile or not.
A little searching on the internet makes it appear to me that this hasn’t been done, so over the next week or so I will attempt it. Prior to setting out, 
here are a few thoughts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Where the hell do I even get a dataset from?&lt;/li&gt;
  &lt;li&gt;Are we recording enough information about the relationship?&lt;/li&gt;
  &lt;li&gt;Is a max-margin approach really going to give us the fine-grained control we want?&lt;/li&gt;
  &lt;li&gt;How do I tell if this was a waste of time?&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;fine-whatever&quot;&gt;Fine. Whatever&lt;/h1&gt;

&lt;p&gt;As usual, I predict the first of these things will take the most of my concentration. That’s just how this machine learning caper works.
More on this next time.&lt;/p&gt;

&lt;p&gt;The second thing makes me wonder a bit. Normally when building word vectors, you work with co-occurrence of terms rather than explicitly with synonym relationships as implied by how FaceNet works.
I wonder if this is really sensible, as it may constrain the types of information that can be encoded in a vector. That said, would this prove more useful for classification tasks?
That seems to be one of the primary uses for FaceNet embeddings and they’re very, very handy in that setting.&lt;/p&gt;

&lt;p&gt;Thirdly, the max-margin thing. The classic way of explaining why word vectors are worth the bother involves adding and subtracting them to then &lt;a href=&quot;https://twitter.com/goodfellow_ian/status/1133528189651677184?lang=en&quot;&gt;pretend they give you a reasonably intuitive answer&lt;/a&gt;.
This property could potentially be disturbed by max-margin learning, because within a given set of synonyms the learning process wouldn’t necessarily make any attempt
at organising the individual elements. On the other hand, a thesaurus can be regarded as a really massive pile of partially-overlapping sets, so this may not actually be a problem after all.&lt;/p&gt;

&lt;p&gt;The last problem is the most annoying, because this boils down to qualitative analysis. I’ve got another whole blog post in mind about why qualitative analysis is a minefield and what can be done about it,
but for now let’s just say that the problem here is that we can’t necessarily say definitively whether one set of embeddings is better than another.&lt;/p&gt;

&lt;p&gt;Anyway, that’s what I’m going to try - very curious to see how it turns out!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;If you want a simpler explanation of how FaceNet works, &lt;a href=&quot;https://omoindrot.github.io/triplet-loss&quot;&gt;check this out&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">The problem of facial recognition is now pretty much solved thanks to FaceNet.1 The underpinning insight that makes it all come together is analogous to the way an SVM works. But have we done all we can with this rather novel form of network? If you want a simpler explanation of how FaceNet works, check this out. &amp;#8617;</summary></entry><entry><title type="html">What does “Manias, Panics and Crashes” teach us about ourselves?</title><link href="http://localhost:4000/2019/08/16/Manias_Panics_and_Crashes.html" rel="alternate" type="text/html" title="What does &quot;Manias, Panics and Crashes&quot; teach us about ourselves?" /><published>2019-08-16T00:00:00+10:00</published><updated>2019-08-16T00:00:00+10:00</updated><id>http://localhost:4000/2019/08/16/Manias_Panics_and_Crashes</id><content type="html" xml:base="http://localhost:4000/2019/08/16/Manias_Panics_and_Crashes.html">&lt;p&gt;One book I’ve been reading with a fair bit of interest recently is &lt;em&gt;Manias, Panics and Crashes&lt;/em&gt;, by the late Charles P. Kindleberger. 
It’s a great read if you’ve got the stomach for reading about finance and is rightly a great of the genre in my view.&lt;/p&gt;

&lt;p&gt;After making it a certain way in though, it becomes obvious that tales of swindling and finance are really just window dressing for the real subject matter.
Many of the things in the book boil down to group psychology leading us in bad directions. Which is interesting in itself; bank runs are a perfect example of this.
Bank runs are triggered by a collective loss of confidence in the ability of a bank to redeem deposits, yet also precisely create that same difficulty. 
Leaving aside the obvious critique of supposedly infallible markets, this makes you wonder; in what other circumstances does one see this kind of thing?&lt;/p&gt;

&lt;p&gt;Politics is one answer. Sport is another.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; It can happen in daily life, too. This got me wondering whether the approach to dealing with a financial panic sheds any light on how to avoid one personally.
It seems like the most obvious advice is also the least enlightening: &lt;em&gt;don’t set yourself up for this in the first place&lt;/em&gt;. It’s also probably the hardest to implement in my opinion,
because it’s quite difficult to continually live your life detached from the moment. Sometimes it simply doesn’t work like that, even. This isn’t to say that thinking about
the lessons of herd behaviour from history can’t help us as individuals, but it’s not as simple as acting like a central bank if you’re going off-course.&lt;/p&gt;

&lt;p&gt;Another thought that arises from the book is also worth remarking upon. It suggests that the incidence of fraud increases during bubble times because people want their wealth to grow &lt;em&gt;even&lt;/em&gt; faster. In turn, 
the book also says the incidence of fraud grows in hard times because people feel driven to it by necessity. So when does the incidence of fraud go &lt;em&gt;down&lt;/em&gt;?
If ever there was an inadvertent critique of our times I thought emblematic, it’s the one written in a mainstream history of capitalism that suggests that the incidence of fraud &lt;em&gt;almost always increases&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So that’s all good, then!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Ever seen your team choke because they collectively got too worried about protecting a lead? Boy, have I ever. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">One book I’ve been reading with a fair bit of interest recently is Manias, Panics and Crashes, by the late Charles P. Kindleberger. It’s a great read if you’ve got the stomach for reading about finance and is rightly a great of the genre in my view.</summary></entry><entry><title type="html">A Hack to Avoid Labelling Topic Classification Data</title><link href="http://localhost:4000/2019/07/27/Topic_Generation_Quickly.html" rel="alternate" type="text/html" title="A Hack to Avoid Labelling Topic Classification Data" /><published>2019-07-27T00:00:00+10:00</published><updated>2019-07-27T00:00:00+10:00</updated><id>http://localhost:4000/2019/07/27/Topic_Generation_Quickly</id><content type="html" xml:base="http://localhost:4000/2019/07/27/Topic_Generation_Quickly.html">&lt;p&gt;Some of the tricks outlined in earlier posts add up to something that I’ve found useful. Specifically, they form a way of generating topic labels and from there classification models.
In this post I’ll attempt to explain how it works.&lt;/p&gt;

&lt;h1 id=&quot;the-set-up&quot;&gt;The Set-up&lt;/h1&gt;

&lt;p&gt;Imagine you’ve got a large amount of free text data, with no labels for the documents. You want to be able to classify them, so what can you do? 
Here are your options:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Brain.png&quot; width=&quot;400&quot; height=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These options are ordered according to the amount of labour involved, with manual annotation the most and blogging the least.&lt;/p&gt;

&lt;h1 id=&quot;manually-annotating-everything&quot;&gt;Manually Annotating Everything&lt;/h1&gt;

&lt;p&gt;Urgh. At least get Mechanical Turk to do it or something. And while you’re at it, &lt;a href=&quot;https://fairwork.stanford.edu/&quot;&gt;it’s not hard to pay your workers fairly&lt;/a&gt;. This is obviously the most labourious choice, which causes a few problems. Firstly, it’s going to take you ages to actually get to the point where you know if this is working or not. Secondly, a lot of examples won’t really be useful because they’re a long way from your (hoped-for) decision boundary and therefore don’t really play much of a role in training. You’ll probably also run the risk of procrastinating, thereby lengthening an already unnecessarily long task.&lt;/p&gt;

&lt;p&gt;There’s another sleeper problem here, as well. How do you know what the topics are before you start labelling? This is a killer. It’s easy to get partway through labelling a large number of documents only to realise your taxonomy is wrong, necessitating a restart.&lt;/p&gt;

&lt;h1 id=&quot;semi-supervised-learning&quot;&gt;Semi-Supervised Learning&lt;/h1&gt;

&lt;p&gt;This is a bit better. Instead of labelling everything, pick some stuff that represents a few topics you’re interested in and try to capture the latent structure of all the unlabelled data when you build a model. This involves potentially a lot less manual labour than the previous option, but there’s still a fair bit of work to do. There are other problems as well.&lt;/p&gt;

&lt;p&gt;As with full manual annotation, you’re still stuck with the task of working out what is actually &lt;em&gt;in&lt;/em&gt; your data before manually labelling anything. Which is still the same chicken-and-egg problem. Another problem lurks in the question: how much is actually enough?
Finally, semi-supervised learning is a bit tricky because of the weird likelihood function among other things. That said, on this last point &lt;a href=&quot;https://breakitdownto.earth/2019/07/12/Semi-Supervised_Learning_Trick.html&quot;&gt;shortcuts do exist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re feeling really advanced, you might try some sort of active learning after you’ve labelled some examples. In my experience this doesn’t work out as planned, because unless you’ve labelled heaps of data everything still looks novel to a classifier. You wind up just labelling all the examples you send to active learning. So that’s not really a shortcut either. Still, at least with this option you don’t necessarily do &lt;em&gt;as much&lt;/em&gt; manual labelling.&lt;/p&gt;

&lt;h1 id=&quot;multi-part-modelling-weirdness&quot;&gt;Multi-part Modelling Weirdness&lt;/h1&gt;

&lt;p&gt;Given that we need some idea of topics before we start labelling them, how can we achieve this? There are plenty of unsupervised tools out there, but let’s look at LDA (&lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot;&gt;Latent Dirichlet Allocation&lt;/a&gt;, not &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_discriminant_analysis&quot;&gt;Linear Discriminant Analysis&lt;/a&gt;) for a second.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;When looking for topics in LDA, you have access to a parameter usually denoted &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;. This governs how broadly-defined the topics you’re looking for are. What you might consider doing is setting that to be something narrow (i.e. 0.8-0.9) and then using that to arrange your documents into small, similar clumps.&lt;/p&gt;

&lt;p&gt;So far so good. Now for each topic you can just inspect a few of the most strongly-associated documents and decide what it’s about. This is still manual labour in a way, but not nearly as onerous and you get a lot more bang for your buck. For each document in, say, the top 10% by score of a given topic, label it with that topic. With an idea of what your topics are about you can put a few of them together in a class, giving rise to large amount of coherently labelled data with comparatively little work.&lt;/p&gt;

&lt;p&gt;Now what you’re left with is a bunch of documents where you can be confident of their class membership and a much larger body that may or may not be members of the classes you’ve created. Which is basically the definition of PU-learning, so &lt;a href=&quot;https://breakitdownto.earth/2019/07/12/Semi-Supervised_Learning_Trick.html&quot;&gt;the trick I alluded to earlier&lt;/a&gt; applies here.&lt;/p&gt;

&lt;p&gt;Long story short, if you use LDA to identify groupings of documents, you can turn them into classification labels easily. Then it’s just a matter of training your model in the right way. Simple!&lt;/p&gt;

&lt;h1 id=&quot;writing-a-blog-about-it&quot;&gt;Writing a Blog About It&lt;/h1&gt;

&lt;p&gt;Obviously the easiest bit. Ta da!&lt;/p&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Some of the tricks outlined in earlier posts add up to something that I’ve found useful. Specifically, they form a way of generating topic labels and from there classification models. In this post I’ll attempt to explain how it works.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/Brain.png" /></entry><entry><title type="html">Music as History</title><link href="http://localhost:4000/2019/07/19/Music_as_History.html" rel="alternate" type="text/html" title="Music as History" /><published>2019-07-19T00:00:00+10:00</published><updated>2019-07-19T00:00:00+10:00</updated><id>http://localhost:4000/2019/07/19/Music_as_History</id><content type="html" xml:base="http://localhost:4000/2019/07/19/Music_as_History.html">&lt;p&gt;A big part of why music is interesting to me derives from the way in which it illustrates the culture that brought it forward. To digress, and delve into the world of Iain M. Banks again, one of his &lt;em&gt;Culture&lt;/em&gt; books has the following to say&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;     Most civilisations that had acquired the means to build genuine Artificial Intelligences duly built them, and most of those designed or shaped the consciousness of the AIs to a greater or lesser extent; obviously if you were constructing a sentience that was or could easily become much greater than your own, it would not be in your interest to create a being which loathed you and might be likely to set about dreaming up ways to exterminate you.&lt;/p&gt;

  &lt;p&gt;     So AIs, especially at first, tended to reflect the civilisational demeanour of their source species. Even when they underwent their own form of evolution and began to design their successors - with or without the help, and sometimes the knowledge, of their creators - there was usually still a detectable flaour of the intellectual character and the basic morality of that precursor species present in the resulting consciousness. That flavour might gradually disappear over subsequent generations of AIs, but it would usually be replaced by another, adopted and adapted from elsewhere, or just mutate beyond recognition rather than disappear altogether.&lt;/p&gt;

  &lt;p&gt;     What various Involveds including the Culture had also tried to do, often out of sheer curiosity once AI had become a settled and even routine technology, was to devise a consciousness with no flavour; one with no metalogical baggage whatsoever; what had become known as a perfect AI.&lt;/p&gt;

  &lt;p&gt;     It turned out that creating such intelligences was not particularly challenging once you could build AIs in the first place. The difficulties only arose when such machines became sufficiently empowered to do whatever they wanted to do. They didn’t go berserk and try to kill all about them, and they didn’t relapse into some blissed-out state of machine solipsism.&lt;/p&gt;

  &lt;p&gt;     What they did do at the first available opportunity was Sublime, leaving the material universe altogether and joining the many beings, communities and entire civilisations which had gone that way before. It was certainly a rule and appeared to be a law that &lt;em&gt;perfect AIs always Sublime&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This passage really grabs me as it makes a very clear link between personal and civilisational identities.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; This is an issue that recurs in a number of contexts outside of award-winning science fiction, as it happens. In this post, I’m going to attempt to talk semi-coherently about this with regards to music.&lt;/p&gt;

&lt;h1 id=&quot;dancing&quot;&gt;Dancing&lt;/h1&gt;

&lt;p&gt;Eurovision makes for great popcorn viewing. The costumes and music frequently boil down to outlandish attempts at gaining viewer attention because of the way the contest is structured. Citizens of European countries vote for the competition winner, so there are well-known voting blocs that have arisen due to cultural affinity.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Despite the obvious incongruity of some of the voting patterns, in a way this kind of makes sense. It makes sense that the path to victory is going to involve either a) triggering existing neural pathways in listeners by presenting something that is familiar to a lot of them (difficult in a diverse population like Europe’s) or b) trying something highly novel to achieve the same end. Or something that tries to do both.&lt;/p&gt;

&lt;p&gt;Verka Serduchka is my favourite example of the second strategy, seen here on the stage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/verka-serd2.jpg&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The play-stuff-they-know strategy is kind of interesting and might help explain why voting blocs exist. The more angles you have for understanding something, the easier it is to recall. So if a song connects with more parts of your memory, you’re likely to remember it more when it comes to voting.&lt;/p&gt;

&lt;p&gt;This isn’t really that big of a point, but really just a stepping stone towards a half-formed idea about how music tells you something about the circumstances in which it was made. More on this in the future!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;em&gt;Look to Windward&lt;/em&gt;, Iain M. Banks, London: Orbit, 2000, ISBN 1-85723-981-4 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;This is something that Mihalyi Csikszentmihalyi has a lot to say about in &lt;em&gt;Flow&lt;/em&gt;, which I’ll revisit at another time. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;https://en.wikipedia.org/wiki/Voting_at_the_Eurovision_Song_Contest#Regional_bloc_voting It’s hilarious how some of these blocs consist of countries that HATE each other. The Balkan states as a voting bloc?! &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">A big part of why music is interesting to me derives from the way in which it illustrates the culture that brought it forward. To digress, and delve into the world of Iain M. Banks again, one of his Culture books has the following to say1: Look to Windward, Iain M. Banks, London: Orbit, 2000, ISBN 1-85723-981-4 &amp;#8617;</summary></entry><entry><title type="html">How to avoid doing semi-supervised learning</title><link href="http://localhost:4000/2019/07/12/Semi-Supervised_Learning_Trick.html" rel="alternate" type="text/html" title="How to avoid doing semi-supervised learning" /><published>2019-07-12T00:00:00+10:00</published><updated>2019-07-12T00:00:00+10:00</updated><id>http://localhost:4000/2019/07/12/Semi-Supervised_Learning_Trick</id><content type="html" xml:base="http://localhost:4000/2019/07/12/Semi-Supervised_Learning_Trick.html">&lt;p&gt;It’s easy to find yourself solving a problem with labels for only &lt;em&gt;some&lt;/em&gt; of your data. This creates a quandary - what to do about the rest of your unlabelled data? There are a few possible answers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ignore it and just work with what you have&lt;/li&gt;
  &lt;li&gt;Try to include it, but:
    &lt;ul&gt;
      &lt;li&gt;Do so without making any guess about what the true label is&lt;/li&gt;
      &lt;li&gt;Try to infer the missing labels first&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you’d guess, all of the above are a nightmare. If you ignore your unlabelled data, you’re setting up an obvious bias problem. If you try to include unlabelled data as-is the math starts getting complicated. And trying to infer the missing labels first can create noise if you’re not careful. So how can we get out of all of this? This post is about one possible answer.&lt;/p&gt;

&lt;h1 id=&quot;the-set-up&quot;&gt;The Set-Up&lt;/h1&gt;

&lt;p&gt;Incorporating unlabelled data in a model falls under the heading of “semi-supervised” learning. One particular example of this is “positive-unlabelled” learning. This consists of having labels for some positive examples and many unlabelled examples. This is a pretty common scenario and one I’ve found myself in before.&lt;/p&gt;

&lt;p&gt;Semi-supervised learning is useful but complex, so it’s better to achieve the same thing by simpler means. If you wanted to go down that path, you’d have to code up a fairly complicated likelihood. Instead, it turns out that you can have basically the same gradient without that. Thanks to &lt;a href=&quot;http://www.machinedlearnings.com/2012/03/pu-learning-and-auc.html&quot;&gt;Paul Mineiro pointing this paper out&lt;/a&gt;, we know that the AUC of a full-labelled model is proportionate to one with only positive labels available. As he says, this means you can just treat unlabelled examples as members of the negative class. Once you’ve done that, all you need to do is optimise AUC.&lt;/p&gt;

&lt;h1 id=&quot;optimising-auc&quot;&gt;Optimising AUC&lt;/h1&gt;

&lt;p&gt;Optimising AUC is tricky because there’s no gradient to work with. The simplest definition of it boils down to repeated random sampling from the positive and negative classes. The obvious thing to do is employ an age-old trick, summed up as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“When you don’t know how to solve something, try to solve something else”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In AUC terms, this means adopting a pairwise loss or ranking-based metric. Parwise loss metrics generally mimic the kind of thing that AUC is trying to characterise. If you’re lucky enough to be working with something like Tensorflow, then these are readily available. If you go down this path, then it’s a simple matter of just altering regularisation parameters and learning rates to taste.&lt;/p&gt;

&lt;p&gt;You might also instead use whatever loss function you were planning to use but just keep track of AUC anyway. One final thought with regards to this is that any metric you can get from the 2x2 confusion matrix can be optimised by altering the class weights. AUC is based on the true positive and false positive rates, so it falls into this category.&lt;/p&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">It’s easy to find yourself solving a problem with labels for only some of your data. This creates a quandary - what to do about the rest of your unlabelled data? There are a few possible answers:</summary></entry><entry><title type="html">What I learnt about Flow from playing Sudoku</title><link href="http://localhost:4000/2019/07/05/Sudoku_and_Flow.html" rel="alternate" type="text/html" title="What I learnt about Flow from playing Sudoku" /><published>2019-07-05T00:00:00+10:00</published><updated>2019-07-05T00:00:00+10:00</updated><id>http://localhost:4000/2019/07/05/Sudoku_and_Flow</id><content type="html" xml:base="http://localhost:4000/2019/07/05/Sudoku_and_Flow.html">&lt;p&gt;Flow is another concept I’ve been interested in for a while. I only started really focussing on it when I realised there could be an overlap with some of the other things I’m interested in, such as &lt;a href=&quot;https://breakitdownto.earth/2019/06/20/Things_Ive_Learnt_From_Duolingo.html&quot;&gt;habit building&lt;/a&gt;. In this post, I’ll talk about where thinking about this explicitly has led for me.&lt;/p&gt;

&lt;p&gt;Flow is kind of an interesting concept - in a nutshell, it reflects the hazy idea that you can get “in the zone”&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; with greater or lesser ease and that the conditions for doing so are worth thinking about. The first thing that got me onto this was forever ago - as in, teenage years forever ago.&lt;/p&gt;

&lt;h1 id=&quot;if-you-never-ever-try-youll-never-ever-know&quot;&gt;If you never ever try, you’ll never ever know&lt;/h1&gt;

&lt;p&gt;When I was a lot younger, I used to work as a shelf stacker at a supermarket. It’s still one of the best jobs I’ve ever had. You’d turn up at an ungodly hour and repeat the same task, several times a week. So in the absence of any clocks, how are you supposed to keep on track?&lt;/p&gt;

&lt;p&gt;After getting told off a couple of times for being slow, I realised that I was at my fastest when listening to relatively fast-paced music, so I stuck to that. Then after a month or two of listening to the same album every time I went to work, I realised that you could use it as a timer of sorts. The music playing told you roughly where you needed to be at that point in time. This was pretty handy as it also helped with something else needed to do a repetitious job effectively: being able to blank your mind.&lt;/p&gt;

&lt;h1 id=&quot;the-remedy&quot;&gt;The remedy&lt;/h1&gt;

&lt;p&gt;Being able to blank out came in handy when playing sudoku many years later. Sudoku is a pretty interesting game: you can get a computer to solve it in the blink of an eye because it’s ultimately not that complex. For a human on the other hand, it can appear fiendishly difficult because there are too many things to think about.&lt;/p&gt;

&lt;p&gt;That’s actually misleading, as it turns out. The way to get good at Sudoku (and increase your odds of achieving a state of flow), is to think about less rather than more. There are a number of simple strategies you can use to identify tiles on the board that have only one option left. They range from simple to fairly complex, but all of them if correctly applied will eventually lead you to filling in tiles.&lt;/p&gt;

&lt;p&gt;Some of the strategies you can use include attempting to identify all the remaining tiles of one type, searching for almost-complete rows and columns and more complex things such as identifying possible locations in a row or column an unplaced number could go. The trick to executing them effectively is actually to keep things simple. If you start trying to go for a trick shot and doing a few different things at the same time, you’ll fall over quickly. But if you run through one strategy mechanically, then the next and the next then you’ll do well.&lt;/p&gt;

&lt;h1 id=&quot;magic-people&quot;&gt;Magic people&lt;/h1&gt;

&lt;p&gt;This is where flow starts to come in. One of the keys to achieving a state of flow relates to the difficulty involved. If you can get it right - not so easy you get bored, not so hard it’s impossible - then you can find yourself in a state where everything feels almost effortless and almost instinctive. That’s flow.&lt;/p&gt;

&lt;p&gt;Fortunately, the average Sudoku app caters to this nicely with a difficulty setting. At first I wasn’t really thinking about this, I was simply trying to incrementally improve by putting up the difficulty whenever my solution times started dropping. Then once I got to the Hard setting I started realising that I wasn’t really learning that much new but sometimes would fall into flow.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It then became apparent that this was most likely to happen when I was faithfully applying the strategies I’d learned rather than trying anything new. That’s interesting, because it suggests to me that flow and learning something aren’t really compatible which shouldn’t necessarily be the case.&lt;/p&gt;

&lt;p&gt;The other thing I noticed was that when I played Sudoku more regularly, I tended to be more likely to achieve a state of flow.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This is interesting, because it suggested to me that there might be a link between habits and flow that’s a little deeper than I thought. I wonder if there’s some way of exploring this a bit more explicitly.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Ugh. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;You feel like a magician immediately after. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;It wasn’t just a case of it being more frequent because I played more. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Flow is another concept I’ve been interested in for a while. I only started really focussing on it when I realised there could be an overlap with some of the other things I’m interested in, such as habit building. In this post, I’ll talk about where thinking about this explicitly has led for me.</summary></entry><entry><title type="html">Another way to look at risk versus reward in binary classification</title><link href="http://localhost:4000/2019/06/28/Risk_in_Classification.html" rel="alternate" type="text/html" title="Another way to look at risk versus reward in binary classification" /><published>2019-06-28T00:00:00+10:00</published><updated>2019-06-28T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/28/Risk_in_Classification</id><content type="html" xml:base="http://localhost:4000/2019/06/28/Risk_in_Classification.html">&lt;p&gt;Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production.&lt;/p&gt;

&lt;p&gt;It’s easy to see that you could wind up like this:&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/this_is_fine.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So why don’t we try and make a guess at how much lower our performance might turn out to be? In this post, I’ll look at one way of doing this with a binary classifier. If you don’t want to worry about the details, &lt;a href=&quot;#the-bigger-picture&quot;&gt;skip to the final section&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course, the worst case is that your model gets everything wrong. That’s way off in the tails of your likely outcomes and not the point of this exercise, though. What I’m aiming for here is a way of looking at how much a model might underperform in reality.&lt;/p&gt;

&lt;h1 id=&quot;gotta-risk-it-to-get-the-biscuit&quot;&gt;Gotta risk it to get the biscuit&lt;/h1&gt;

&lt;p&gt;Before you do anything, you need a decent baseline to compare to. What might we do in the absence of a model? You could flip a coin. Or you could always predict the commonest class. You could even use another much simpler model.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;For now, let’s say that your baseline model is constant-valued. The model predicts all data are from the majority class.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; So what does this mean for performance?&lt;/p&gt;

&lt;p&gt;Imagine you’ve got a dataset where 90% of the labels are negative and the remaining 10% are positive. The constant-valued classifier gives you 90% accuracy. So if you do any worse than that on a model you’ve built, it’s fair to claim that you’ve wasted your time.&lt;/p&gt;

&lt;p&gt;This is where things get interesting. We can be pretty confident about that 90% accuracy figure, because the model is simple. The more complex your model is though, the more open you are to things getting funky.&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;So it’s reasonable to ask yourself:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Given the extra performance above a baseline model I see, how much risk do I take in exchange?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;…because after all, there ain’t no such thing as a free lunch!&lt;/p&gt;

&lt;h1 id=&quot;down-on-the-upside&quot;&gt;Down on the upside&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;

&lt;p&gt;It’s easy to work out how much upside you’re capturing relative to a baseline. All you do is subtract the baseline accuracy you’ve got from your model’s reported test accuracy. If that comes out negative, stop reading: you have bigger problems.&lt;/p&gt;

&lt;h1 id=&quot;this-is-where-it-gets-confusing&quot;&gt;This is where it gets confusing&lt;/h1&gt;

&lt;p&gt;Now how much risk are you taking on to get the outperformance you see? One way of looking at this involves &lt;a href=&quot;https://breakitdownto.earth/2019/06/12/Automated_Mistakes_with_AutoML.html#confused-yet&quot;&gt;model perplexity (brief explanation you should read here)&lt;/a&gt;. The question is, how to arrive at that measure for a binary classifier. The good news is that it’s easy to turn binary cross-entropy into a perplexity score.&lt;/p&gt;

&lt;p&gt;Wikipedia informs us that&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The perplexity of a discrete probability distribution p is defined as:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;2^{H(p)}=2^{-\sum _{x=1} p(x)\log _{2}p(x)}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;The bit on the left side is the important thing here. Perplexity is equal to a base raised to the power of entropy. The base needs to be the same as the one used calculating entropy. So if you used the natural log when calculating cross-entropy, all you need to do is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;perplexity = e^{loss}&lt;/script&gt;

&lt;p&gt;and you’ve got a perplexity score. Perplexity can become an estimated accuracy score, too. If you know that only a couple of outcomes are plausible, then at worst you can make a random guess among them. This means that all you have to do is this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ACC_{lowerbound} = \frac{1}{e^{loss}}&lt;/script&gt;

&lt;p&gt;So just exponentiate your loss and then divide one by it. Done.&lt;/p&gt;

&lt;h1 id=&quot;the-bigger-picture&quot;&gt;The bigger picture&lt;/h1&gt;

&lt;p&gt;So we now have three numbers to work with:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The accuracy as reported on our test set&lt;/li&gt;
  &lt;li&gt;The accuracy achieved by a baseline model&lt;/li&gt;
  &lt;li&gt;The estimate we have of how badly our model might plausibly perform in the wild derived above&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now it’s just a matter of judgement really. Given how well your model appears to perform, is the risk on the downside worth it?&lt;/p&gt;

&lt;p&gt;Here’s a scenario. You have a model with the following results:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reported test set accuracy of 93% (implying 3% of upside on a baseline classifier)&lt;/li&gt;
  &lt;li&gt;A baseline accuracy of 90%&lt;/li&gt;
  &lt;li&gt;A lower bound estimate for accuracy of 75% (implying quite a lot of downside)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It looks like the model takes on quite a bit of risk for the upside it appears to be capturing, so you may think twice about putting this model into production. It’s not hard to think about another situation where the worst case accuracy is not far below a random baseline. Another situation I’ve faced looked like this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reported accuracy of 92% (5% above baseline)&lt;/li&gt;
  &lt;li&gt;Baseline accuracy of 87%&lt;/li&gt;
  &lt;li&gt;Lower bound estimate of 84% (3% below)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;which seemed more favourable.&lt;/p&gt;

&lt;p&gt;None of this is all that revolutionary. The aim is to think explicitly about how much trouble a model might cause in production. That’s really the big deal here. Treating models like software means being clear-eyed about how they fail like software. With any luck, this post will help someone with that.&lt;/p&gt;

&lt;p&gt;Finally, you can do this analysis with other metrics aside from accuracy with some effort. Multi-class problems can also be addressed with a certain amount of jiggery pokery.&lt;/p&gt;

&lt;p&gt;Hooray!&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Big caveat with this last one. If you’re not respecting the word “simple”, it probably shouldn’t be a baseline. The idea is to work out how much better off you are compared to no model. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;You could use something else, though. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;This is basically how adversarial attacks on ML models have become a thing. The more complicated models get, the more room there is for this. But this is an aside. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Yup. Soundgarden jokes. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;https://en.wikipedia.org/wiki/Perplexity &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Models in production are software, so why don’t we think about how badly they might perform? When we put a model into production we tend to take test accuracy/F1/AUC as fixed, even though we generally know that’s not true. Test set performance is an estimate, not a guarantee, of what you’ll see in production.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/this_is_fine.png" /></entry><entry><title type="html">Things I’ve learnt from one-and-a-bit years of Duolingo</title><link href="http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo.html" rel="alternate" type="text/html" title="Things I've learnt from one-and-a-bit years of Duolingo" /><published>2019-06-20T00:00:00+10:00</published><updated>2019-06-20T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo</id><content type="html" xml:base="http://localhost:4000/2019/06/20/Things_Ive_Learnt_From_Duolingo.html">&lt;p&gt;I’ve been using Duolingo for about 470 days now to learn Chinese. There are a few things I’ve learned on the way, but mostly I’ve learned to apply the lessons of a book I read partway through this journey. In this post, I’ll talk about that and what it leads me to think about how to turn this blog into a durable habit.&lt;/p&gt;

&lt;h1 id=&quot;pretty-clear-i-guess&quot;&gt;Pretty clear, I guess&lt;/h1&gt;

&lt;p&gt;The book is &lt;em&gt;Atomic Habits&lt;/em&gt; by James Clear.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The basic premise of the book is that we can improve our lives quite a lot by making minor changes that stack up over time and that the best way to do that involves reducing our reliance on motivation and will.&lt;/p&gt;

&lt;p&gt;All pretty intuitive so far, but what does this mean in practice? Well:&lt;/p&gt;

&lt;p&gt;1) You need to make the thing you want to do the default option, in some sense
2) You need to get yourself in a situation where you don’t have to go to a gigantic amount of effort each time in order to build a habit&lt;/p&gt;

&lt;p&gt;The book also suggests that you need some kind of reward mechanism for engaging in the habit you’re trying to build. I found that unnecessary, for reasons I’ll expand upon in a little bit.&lt;/p&gt;

&lt;h1 id=&quot;first-things-first&quot;&gt;First things first&lt;/h1&gt;

&lt;p&gt;How did I make it the default option? Well that’s easy, I just put the app icon for Duolingo in a really prominent place on my phone and decided that I would do it every day after brushing my teeth in the evening. So far so good. Thing is, this is designed to work really weell for a simple, bite-sized task that can be done based on a trigger like brushing your teeth.&lt;/p&gt;

&lt;p&gt;Even better, you can do it &lt;em&gt;while&lt;/em&gt; brushing your teeth. This is trickier though.&lt;/p&gt;

&lt;p&gt;What about a blog? Writing these posts actually takes a little time to think and plan aside from the writing itself, which isn’t necessarily quick either unless I’ve got something I really want to get off my chest. So, here’s where another trick in James Clear’s book helps.&lt;/p&gt;

&lt;p&gt;I’ve gotten into an extremely foolhardy bet with &lt;a href=&quot;https://lockwood.dev&quot;&gt;my friend Tom&lt;/a&gt;, which involves us both writing a blog post each week and seeing who can achieve greater reach, with the winner receiving a specified treat from our local café. Unfortunately given that he has suddenly turned into an opinion writer with no shortage of ideas I’m probably in a lot of trouble with regards to that bet. Fortunately, that’s not the point.&lt;/p&gt;

&lt;p&gt;What is the point is that now we have a goal to work towards. This alleviates part of the problem with turning something that isn’t bite-sized into a habit by giving a clear definition of whether or not the desired action has occurred. The problem now is one of how to make it reasonably automatic within that time frame.&lt;/p&gt;

&lt;h1 id=&quot;unproductively-wasting-time&quot;&gt;Unproductively wasting time&lt;/h1&gt;

&lt;p&gt;One problem I stumbled upon with building a Duolingo habit is that sometimes you’re just going through the motions. When you do the exercises every day, it’s inevitable that to some degree you rote learn the content. Don’t worry about breaking that just now, but instead figure out how to make sure you can learn something on top of it. It doesn’t even have to be much, but in any event you don’t want to come away from a Duolingo session having successfully avoided any mental exertion at all (which we are of course so good at avoiding).&lt;/p&gt;

&lt;p&gt;One thing you can do is make yourself say out loud either the question or answer from one question from each exercise. Could just be the first, to make it easy and get it over with. But that way you can’t do a non-zero amount of thinking in the worst case.&lt;/p&gt;

&lt;p&gt;Similarly with a blog. The challenge presents slightly differently. It’s very easy to want to write a blog post with little or no content in it, in order to just tick the box. So how do you make sure that &lt;em&gt;in the worst case&lt;/em&gt;, you haven’t just written a bunch of words? I don’t have a great answer for this yet, unfortunately, given that I’ve only been writing this blog for a little bit.&lt;/p&gt;

&lt;p&gt;There are a couple of things that point me in the direction of a solution, though. The first is to cut it down. &lt;a href=&quot;https://breakitdownto.earth/2019/06/06/Obfuscating_a_lack_of_AI.html&quot;&gt;My most widely-read post so far&lt;/a&gt; is actually perhaps the simplest one I’ve written. It’s very tempting to pack more and more thoughts into a post and wind up feeling like you’re going to too much effort. Instead just write the whole thing down as one sentence. Then turn that into a post.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Biting of more than you can chew is the fastest way to turn the exercise of writing a blog post into a box-ticking exercise, because you start minimising the effort on each part of the post and the whole suffers as a result.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It’s also worth taking the effort off yourself to remember a coherent argument when you sit down to write someting. So any time I have an idea for a post, I immediately create a draft with a single-sentence summary and a suggested publication date in the file name. That way, I have a few ideas I can work on and a way of prioritising which gets done. If you only write a bit of a post in one session, then that’s fine. By having a few being pushed forward at any one time, you don’t have to worry about finding the time for it. Now it’s possible to just write a paragraph of a post according to some schedule and the bigger picture takes care of itself.&lt;/p&gt;

&lt;h1 id=&quot;stuff-that-didnt-matter&quot;&gt;Stuff that didn’t matter&lt;/h1&gt;

&lt;p&gt;Part of the problem with setting out on writing a blog or learning Chinese is the lure of the goal is in a way self-defeating. We want to feel like we’ve accomplished the thing we’re setting out to achieve, but allowing yourself to contemplate that makes it harder because now you’re focusing on the amount of work you have to do &lt;em&gt;rather than just getting it all done&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;James Clear recommends rewarding yourself on successful completion of a task (or someone else in the case of my idiotic bet), but I find that you don’t need to emphasise this too much. Instead it can be easier to make the whole thing unthinking, in a way. Focussing on the goal or what you still want to achieve is demotivating in a way, I’ve found. Instead just try to make sure you get stuff done and that sort of thin has a way of magically taking care of itself.&lt;/p&gt;

&lt;p&gt;The downside is you don’t get the warm and fuzzy feeling involved in getting somewhere, instead it’s more of a mild surprise at saying something accurate in Chinese. Which is its own kind of fun, I guess.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Thanks Tom Roth for putting me onto it. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;I’m very aware that I’m not following this advice with this particular post, but that’s because I’m using this to clarify my own thoughts rather than say anything to anyone. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;This dovetails nicely with the &lt;em&gt;make it easy&lt;/em&gt; dictum from &lt;em&gt;Atomic Habits&lt;/em&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">I’ve been using Duolingo for about 470 days now to learn Chinese. There are a few things I’ve learned on the way, but mostly I’ve learned to apply the lessons of a book I read partway through this journey. In this post, I’ll talk about that and what it leads me to think about how to turn this blog into a durable habit.</summary></entry><entry><title type="html">How to gather a bunch of evidence and then ignore it</title><link href="http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence.html" rel="alternate" type="text/html" title="How to gather a bunch of evidence and then ignore it" /><published>2019-06-15T00:00:00+10:00</published><updated>2019-06-15T00:00:00+10:00</updated><id>http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence</id><content type="html" xml:base="http://localhost:4000/2019/06/15/Unwittingly_Discarding_Evidence.html">&lt;p&gt;Oh my god this happens all the time I swear. If you’re working in machine learning then it is unavoidable that you work in the presence of uncertainty. This isn’t such a bad thing, but how we think as humans sometimes works against us when dealing with uncertainty. This post will look something that commonly goes wrong and more importantly, how to spot it so you can do something about it.&lt;/p&gt;

&lt;p&gt;Everyone’s different&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, so maybe this advice won’t work for everyone. But that’s fine.&lt;/p&gt;

&lt;h1 id=&quot;whats-the-story&quot;&gt;What’s the story?&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;

&lt;p&gt;When building a model it’s easy to get caught in a narrative fallacy. This was described in &lt;em&gt;The Black Swan&lt;/em&gt; by N. N. Taleb as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The problem is, this is really pervasive. And like any kind of absolutely normal background noise, it can sometimes to be hard to spot and focus on.&lt;/p&gt;

&lt;h1 id=&quot;thinking-is-hard-&quot;&gt;Thinking is hard :(&lt;/h1&gt;

&lt;p&gt;Fair enough. Machine learning models can be big complicated things, so knowing what’s going on in them is an active field of research in its own right, not just an onerous task for the practitioner. An easy way to catch yourself succumbing to a narrative fallacy is to look for yourself saying &lt;em&gt;“I know what’s going on here”&lt;/em&gt;, or words to that effect.&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This is because if you’re saying that you’re potentially focussing on the rare delight of being right, as opposed to sticking to the task of working out what you still don’t know. Try getting into the habit of talking about the state of your knowledge without referring to yourself and see how it changes your view.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;I don’t think any less of anyone for thinking like this to be honest, large parts of an ML person’s job consist of being wrong. Wanting a win every now and then is only human. Problem is, a lot of the time when you think you’ve got it all sorted out, you’re actually shooting yourself in the foot at that very moment. It can really be a dirty, difficult job. So keep an eye on yourself if you find yourself desperate for a breakthrough, you might inadvertently join the dots in a way that allows you to hallucinate one.&lt;/p&gt;

&lt;h1 id=&quot;performance-statistic-hell&quot;&gt;Performance statistic hell&lt;/h1&gt;

&lt;p&gt;Turn it up to 11 if you find yourself deliberately glossing over any piece of evidence at your disposal because it conflicts with your understanding of your progress.&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; As a rule, I generally tend to track a number of different metrics when training a model and look for situations where their results conflict. Log loss going down but F1 going down with it? This incongruity is the sort of thing you want to know about, rather than disregarding one of these numbers because it doesn’t fit a proposed explanation about the functioning of your model. Go hunting for a mess in your performance statistics, just don’t over-interpret the evidence you have.&lt;/p&gt;

&lt;p&gt;A handy way to do that is to not let yourself get away from the definitions of the metrics you’re working with. Let’s work through the above example. In a classification setting, log loss is calculated as the logged distance between the prediction and label. So if that on average decreases, fine, predictions are closer to the actual labels &lt;em&gt;on average&lt;/em&gt;. The words “on average” are important because they ignore what your labels are - so shifts in log loss can be explained by an overall shift in the distribution of predictions created by your model. F1 gives you a blended measure of your model’s precision and recall. If F1 goes down, at least one of precision and recall went down. So you should inspect the confusion matrix to see what has changed. If it turns out that your predictions have become biased, it will appear here. If that’s not the case, then you can attach a lower probability to biased predictions being the cause of your results and instead look for outliers skewing your average loss result. This is a very different thought process to picking parts of the model and hypothesising about how they may have influenced your performance statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://breakitdownto.earth/2019/06/12/Automated_Mistakes_with_AutoML.html&quot;&gt;AutoML seems like a good way around all of this, but in my opinion it’s an iron law that in any situation a sufficiently innovative and eager person can always find a way to shoot themselves in the foot. You still need to be clear about what you’re up to.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;dont-believe-the-hype&quot;&gt;Don’t believe the hype&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;

&lt;p&gt;A textbook-definition narrative fallacy I see &lt;em&gt;everywhere&lt;/em&gt; consists of pretending your neural network functions the same way as a human brain and consequently believing that if you replicate your own decision-making process in constructing the network then all will be well. When it inevitably blows up temporarily or gives weird results - because it is in fact not a human brain - all of a sudden it’s very difficult to know what to do because that whole frame of reference constrains thinking. Stop doing this!&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; Of course there are plenty of very, very smart people who use the human brain analogy to educate others about how neural nets work, but that doesn’t mean you should consider it any kind of literal truth when building one yourself. Remember, that analogy is a simplification used for the purposes of education. No-one has an LSTM unit in their head.&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&quot;baby-steps&quot;&gt;Baby steps&lt;/h1&gt;

&lt;p&gt;One simple thing you can do to avoid falling for narrative fallacies is to slow down the process. I get it, your boss is an arsehole and you have to keep it moving. What I’m saying is that if you’re caught in a narrative fallacy, there can often be a strong urge to make multiple changes to your model at the same time, interpret whatever evidence you get as confirmation of the working hypothesis you had and then repeat this process. Again, this can be made worse by the rancid breath of your boss on your neck.&lt;/p&gt;

&lt;p&gt;Even worse, the real danger is that you wind up going around and around in circles without realising it due to a lack of clarity about the reasons for the effectiveness or otherwise of the changes you make, or in the worst case a lack of clarity about what you’ve even tried. Or you can wind up going round and round in circles, only to find a way out of it by means of another fallacious narrative that conveniently explains everything (but isn’t actually right). Occam’s Razor is pretty clear that sometimes the simple explanation can be the wrong one.&lt;/p&gt;

&lt;p&gt;Instead, force yourself to make only one change at a time. This does two things for you:&lt;/p&gt;

&lt;p&gt;1) It makes whether or not your change is a good one much clearer, and
2) It allows you to work out what to do next, because you now know precisely what to ascribe success or failure to.&lt;/p&gt;

&lt;p&gt;If you make three changes to a model, train it, then find the results are better - what was responsible? Like as not there’s nothing that really tells you, so the blanks get filled in subconsciously instead. Look at the functioning of your model at a lower level where possible and leave overarching narratives to themselves. Don’t leave room for the imagination by not trying to keep too much in your head at once and let it be an alarm bell if you find yourself really hankering to make a few changes at the same time. It can definitely feel like you’re moving a lot slower, but that’s because there’s a false economy in the alternative. Trying a scatter gun approach with a bunch of changes might feel like a quicker path to a result but not if you stop and consider all the time spent in that approach not being clear about what’s going on.&lt;/p&gt;

&lt;p&gt;This shouldn’t be news to anyone from a scientific background. But we aren’t all, are we? And even if you are, it’s important to make sure that these habits of mind are explicit, not tacit. It’s the unspoken parts of the train of thought that are at issue here.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;of course &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;…&lt;a href=&quot;https://www.youtube.com/watch?v=Wm54XyLwBAk&quot;&gt;morning glory&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;This isn’t meant to be snark. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;The reason for doing this is that &lt;em&gt;you are not your code&lt;/em&gt;. Keeping some mental distance from your code and models can help you think more freely about them. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Being able to spot yourself doing this and act is the hard part. In all seriousness, have a look at some cognitive behavioural therapy material. It’s not just for depressed people, it can help you work more effectively with your internal monologue and state of mind and not let this sort of thing slide through. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;Yup. Public Enemy jokes. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;If you’ve succeeded in building a model like this, well done. But are you sure you succeeded because you had the right approach? There might be cases where you explicitly do want to build a model to replicate some easily-understood process or object - was that so in your case, or is it the case that thinking that having an overarching narrative was the key to success is actually a narrative fallacy in its own right? That’s how pervasive this sort of thing can be. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Maybe Schmidhuber does. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Stephen Hogg</name></author><summary type="html">Oh my god this happens all the time I swear. If you’re working in machine learning then it is unavoidable that you work in the presence of uncertainty. This isn’t such a bad thing, but how we think as humans sometimes works against us when dealing with uncertainty. This post will look something that commonly goes wrong and more importantly, how to spot it so you can do something about it.</summary></entry></feed>